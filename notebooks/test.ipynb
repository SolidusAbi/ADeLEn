{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "project_dir = os.path.join(os.getcwd(),'..')\n",
    "if project_dir not in sys.path:\n",
    "    sys.path.append(project_dir)\n",
    "from experiments.MNIST import ExperimentMNISTBase, ExperimentADeLEn, ExperimentSupervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "def generate_roc_df(roc_list:list) -> pd.DataFrame:\n",
    "    ''' \n",
    "        Create a DataFrame from a list of roc curves\n",
    "        Args:\n",
    "        -----\n",
    "            roc_list: list\n",
    "                List of N roc curves where N is the number of iterations. It is a \n",
    "                list of tuples of the form (fpr, tpr), where fpr is the false positive\n",
    "                rate and tpr is the true positive rate.\n",
    "        Returns:\n",
    "        --------\n",
    "            roc_df: pd.DataFrame\n",
    "                DataFrame with multiindex\n",
    "    '''\n",
    "    index_names = [\n",
    "        list(map(lambda x: 'It {}'.format(x), np.repeat(np.arange(len(roc_list)), 2) + 1 )),\n",
    "        ['FPR', 'TPR']*len(roc_list)\n",
    "    ]\n",
    "        \n",
    "    tuples = list(zip(*index_names))\n",
    "    index = pd.MultiIndex.from_tuples(tuples)\n",
    "    roc_df = pd.DataFrame(chain.from_iterable(roc_list), index=index)\n",
    "    return roc_df\n",
    "\n",
    "def generate_multi_df(data:list, index_names:list) -> pd.DataFrame:\n",
    "    index_names = [\n",
    "        list(map(lambda x: 'It {}'.format(x), np.repeat(np.arange(len(data)), len(index_names)) + 1 )),\n",
    "        index_names*len(data)\n",
    "    ]\n",
    "\n",
    "    tuples = list(zip(*index_names))\n",
    "    index = pd.MultiIndex.from_tuples(tuples)\n",
    "    return pd.DataFrame(chain.from_iterable(data), index=index)\n",
    "\n",
    "def save_result(roc:list, scores:list, metrics:np.ndarray, config:dict) -> tuple:\n",
    "    '''\n",
    "        Save the results of the experiment\n",
    "        Args:\n",
    "        -----\n",
    "            roc: list\n",
    "                List of roc curves\n",
    "            auc: list\n",
    "                List of AUC scores\n",
    "            config: dict\n",
    "                Configuration of the experiment\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "            roc_df: pd.DataFrame\n",
    "                ROC Cuve dataFrame with multiindex, considering the iterations.\n",
    "            auc_df: pd.DataFrame\n",
    "                DataFrame with the AUC scores\n",
    "    '''\n",
    "\n",
    "    roc_df = generate_multi_df(roc, ['FPR', 'TPR']).T\n",
    "    scores_df = generate_multi_df(scores, ['Normal', 'Anomaly']).T\n",
    "    metrics_df = pd.DataFrame(metrics, columns=['Accuracy', 'Precision', 'Recall', 'F1', 'AUC'])\n",
    "    \n",
    "    roc_df.to_pickle(os.path.join(config['save_result_dir'], 'roc.pkl'))\n",
    "    scores_df.to_pickle(os.path.join(config['save_result_dir'], 'sample_score.pkl'))\n",
    "    metrics_df.to_csv(os.path.join(config['save_result_dir'], 'metrics.csv'))\n",
    "\n",
    "    return roc_df, scores_df, metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the pollution in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.MedMNIST import ExperimentMedMNISTBase, ExperimentADeLEn\n",
    "\n",
    "def pollution_test(ExperimentClass:ExperimentMNISTBase, d=None):\n",
    "    n_iter = 25\n",
    "    seed = 2*np.arange(n_iter, dtype=int) + 42\n",
    "\n",
    "    pollution_exp = [0, .05, .1, .2]\n",
    "    for pollution in pollution_exp:\n",
    "        print(f'Exp. {pollution=}')\n",
    "        iterator = tqdm(\n",
    "                    range(n_iter),\n",
    "                    leave=True,\n",
    "                    unit=\"It.\",\n",
    "                    postfix={\"AUC\": \"%.3f\" % -1},\n",
    "                )\n",
    "\n",
    "        roc, scores = [], []\n",
    "        metrics = np.empty((n_iter, 5)) # acc, prec, rec, f1, auc\n",
    "\n",
    "        for it in iterator:\n",
    "            exp = ExperimentClass(known_anomalies=.1, pollution=pollution, d=d, seed=int(seed[it]))\n",
    "            if it == 0:\n",
    "                config = exp.config()\n",
    "                exp.save_config()\n",
    "            \n",
    "            \n",
    "            auc_score = exp.run()\n",
    "            iterator.set_postfix({\"AUC\": \"%.3f\" % auc_score})\n",
    "\n",
    "            fpr, tpr, roc_auc = exp.roc_curve()\n",
    "            normal_scores, anomaly_scores = exp.score_per_label()\n",
    "            acc, prec, rec, f1 = exp.classification_metrics()\n",
    "            \n",
    "            roc.append((fpr, tpr))   \n",
    "            scores.append((normal_scores, anomaly_scores))\n",
    "            metrics[it] = [acc, prec, rec, f1, roc_auc]\n",
    "            \n",
    "        roc_df, scores_df, metrics_df = save_result(roc, scores, metrics, config)\n",
    "        \n",
    "    return roc_df, scores_df, metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottleneck = [2, 5, 10]\n",
    "for d in bottleneck:\n",
    "    print(f'Exp. {d=}')\n",
    "    roc_df, scores_df, metrics_df = pollution_test(ExperimentADeLEn, d=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = exp.config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 2\n",
    "seed = 2*np.arange(n_iter, dtype=int) + 42\n",
    "\n",
    "pollution_exp = [0, .05, .1, .2]\n",
    "for pollution in pollution_exp:\n",
    "    print(f'Exp. {pollution=}')\n",
    "    iterator = tqdm(\n",
    "                range(n_iter),\n",
    "                leave=True,\n",
    "                unit=\"It.\",\n",
    "                postfix={\"AUC\": \"%.3f\" % -1},\n",
    "            )\n",
    "\n",
    "    roc, scores = [], []\n",
    "    metrics = np.empty((n_iter, 5)) # acc, prec, rec, f1, auc\n",
    "\n",
    "    for it in iterator:\n",
    "        exp = ExperimentADeLEn(known_anomalies=.1, pollution=pollution, seed=int(seed[it]))\n",
    "        if it == 0:\n",
    "            config = exp.config()\n",
    "            exp.save_config()\n",
    "        \n",
    "        \n",
    "        auc_score = exp.run()\n",
    "        iterator.set_postfix({\"AUC\": \"%.3f\" % auc_score})\n",
    "\n",
    "        fpr, tpr, roc_auc = exp.roc_curve()\n",
    "        normal_scores, anomaly_scores = exp.score_per_label()\n",
    "        acc, prec, rec, f1 = exp.classification_metrics()\n",
    "        \n",
    "        roc.append((fpr, tpr))   \n",
    "        scores.append((normal_scores, anomaly_scores))\n",
    "        metrics[it] = [acc, prec, rec, f1, roc_auc]\n",
    "        \n",
    "    roc_df, scores_df, metrics_df = save_result(roc, scores, metrics, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal = scores_df.iloc[:, 0]\n",
    "anomaly_scores = scores_df.iloc[:, 1]\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(normal, bins=10, alpha=0.5, label='Normal')\n",
    "plt.hist(anomaly_scores, bins=10, alpha=0.5, label='Anomaly')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal = scores_df.iloc[:, 2]\n",
    "anomaly_scores = scores_df.iloc[:, 3]\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(normal, bins=10, alpha=0.5, label='Normal')\n",
    "plt.hist(anomaly_scores, bins=10, alpha=0.5, label='Anomaly')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_score, anomaly_score = exp.score_per_label()\n",
    "from matplotlib import pyplot as plt\n",
    "plt.hist(normal_score, bins=10, alpha=0.5, label='Normal')\n",
    "plt.hist(anomaly_score, bins=10, alpha=0.5, label='Anomaly')\n",
    "plt.legend()\n",
    "\n",
    "# reduce the margin of the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, auc = exp.roc_curve()\n",
    "# plot the roc curve\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = {:.2f})'.format(auc))\n",
    "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_df, scores_df, metrics_df = pollution_test(ExperimentSupervised)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
