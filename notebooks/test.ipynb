{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "project_dir = os.path.join(os.getcwd(),'..')\n",
    "if project_dir not in sys.path:\n",
    "    sys.path.append(project_dir)\n",
    "from experiments.MNIST import ExperimentFactory, ExperimentType, ExperimentADeLEn\n",
    "from experiments.utils import generate_roc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "def generate_roc_df(roc_list:list) -> pd.DataFrame:\n",
    "    ''' \n",
    "        Create a DataFrame from a list of roc curves\n",
    "        Args:\n",
    "        -----\n",
    "            roc_list: list\n",
    "                List of N roc curves where N is the number of iterations. It is a \n",
    "                list of tuples of the form (fpr, tpr), where fpr is the false positive\n",
    "                rate and tpr is the true positive rate.\n",
    "        Returns:\n",
    "        --------\n",
    "            roc_df: pd.DataFrame\n",
    "                DataFrame with multiindex\n",
    "    '''\n",
    "    index_names = [\n",
    "        list(map(lambda x: 'It {}'.format(x), np.repeat(np.arange(len(roc_list)), 2) + 1 )),\n",
    "        ['FPR', 'TPR']*len(roc_list)\n",
    "    ]\n",
    "        \n",
    "    tuples = list(zip(*index_names))\n",
    "    index = pd.MultiIndex.from_tuples(tuples)\n",
    "    roc_df = pd.DataFrame(chain.from_iterable(roc_list), index=index)\n",
    "    return roc_df\n",
    "\n",
    "def generate_multi_df(data:list, index_names:list) -> pd.DataFrame:\n",
    "    index_names = [\n",
    "        list(map(lambda x: 'It {}'.format(x), np.repeat(np.arange(len(data)), len(index_names)) + 1 )),\n",
    "        index_names*len(data)\n",
    "    ]\n",
    "\n",
    "    tuples = list(zip(*index_names))\n",
    "    index = pd.MultiIndex.from_tuples(tuples)\n",
    "    return pd.DataFrame(chain.from_iterable(data), index=index)\n",
    "\n",
    "def save_result(roc:list, scores:list, metrics:np.ndarray, config:dict) -> tuple:\n",
    "    '''\n",
    "        Save the results of the experiment\n",
    "        Args:\n",
    "        -----\n",
    "            roc: list\n",
    "                List of roc curves\n",
    "            auc: list\n",
    "                List of AUC scores\n",
    "            config: dict\n",
    "                Configuration of the experiment\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "            roc_df: pd.DataFrame\n",
    "                ROC Cuve dataFrame with multiindex, considering the iterations.\n",
    "            auc_df: pd.DataFrame\n",
    "                DataFrame with the AUC scores\n",
    "    '''\n",
    "\n",
    "    roc_df = generate_multi_df(roc, ['FPR', 'TPR']).T\n",
    "    scores_df = generate_multi_df(scores, ['Normal', 'Anomaly']).T\n",
    "    metrics_df = pd.DataFrame(metrics, columns=['Accuracy', 'Precision', 'Recall', 'F1', 'AUC'])\n",
    "    \n",
    "    roc_df.to_pickle(os.path.join(config['save_result_dir'], 'roc.pkl'))\n",
    "    scores_df.to_pickle(os.path.join(config['save_result_dir'], 'sample_score.pkl'))\n",
    "    metrics_df.to_csv(os.path.join(config['save_result_dir'], 'metrics.csv'))\n",
    "\n",
    "    return roc_df, scores_df, metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "n_iter = 2\n",
    "iterator = tqdm(\n",
    "            range(n_iter),\n",
    "            leave=True,\n",
    "            unit=\"It.\",\n",
    "            postfix={\"AUC\": \"%.3f\" % -1},\n",
    "        )\n",
    "\n",
    "# experiments_type = [ExperimentType.Anomalies_001,ExperimentType.Anomalies_005, ExperimentType.Anomalies_010, ExperimentType.Anomalies_020]\n",
    "# experiments_type = [ExperimentType.Anomalies_010, ExperimentType.Anomalies_020]\n",
    "experiments_type = [ExperimentType.Anomalies_005]\n",
    "\n",
    "for experiment_type in experiments_type:\n",
    "    auc, roc, auc_best_score = [], [], -1\n",
    "    print('Experiment: {}'.format(experiment_type))\n",
    "    for i in iterator:\n",
    "        experiment = ExperimentFactory.create(experiment_type)\n",
    "        if i == 0:\n",
    "            config = experiment.config()\n",
    "            experiment.save_config()\n",
    "        experiment.run()\n",
    "\n",
    "        tpr, fpr, roc_auc = experiment.to_test()\n",
    "        roc.append((tpr, fpr))\n",
    "        auc.append(roc_auc)\n",
    "\n",
    "        iterator.set_postfix(tls=\"%.3f\" % roc_auc)\n",
    "        \n",
    "        if roc_auc > auc_best_score:\n",
    "            auc_score = roc_auc\n",
    "            experiment.save_model(verbose=False)\n",
    "            fig = experiment.plot_reconstructed(experiment.model, r0=(-6, 6), r1=(-6, 6), n=15 )\n",
    "            fig.savefig(os.path.join(config['save_imgs_dir'], f'reconstructed.pdf'), bbox_inches='tight')\n",
    "            plt.close(fig)\n",
    "\n",
    "    roc_df, auc_df = save_result(roc, auc, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = ExperimentFactory.create(ExperimentType.Anomalies_020, seed=42)\n",
    "experiment.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "x, y = zip(*[(_x, _y) for _x, _y in experiment.test_dataset])\n",
    "x = torch.stack(x)\n",
    "y = torch.tensor(y)\n",
    "\n",
    "_ = experiment.model(x)\n",
    "sigma = experiment.model.bottleneck.sigma\n",
    "normalize = np.log(2*torch.pi*torch.e)\n",
    "y_score = 0.5 * (len(sigma) * normalize + torch.log(torch.sum(sigma, dim=1)).detach().numpy())\n",
    "# y_score = normalize + torch.log(torch.prod(sigma, dim=1)).detach().numpy()\n",
    "        \n",
    "fpr, tpr, thresholds = roc_curve(y, y_score)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d, _ = torch.rand(10, 10, 10).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.hist(y_score[y==0], bins=15, alpha=0.5, label='Normal')\n",
    "plt.hist(y_score[y==1], bins=15, alpha=0.5, label='Anomaly')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = torch.tensor([1,1]) \n",
    "y_score_1 = 0.5* (normalize + torch.log(sigma).sum().detach().numpy())\n",
    "y_score_2 = normalize + torch.log(sigma).sum().detach().numpy()\n",
    "y_score_3 = torch.log(sigma).sum().detach().numpy()\n",
    "\n",
    "print(y_score_1, y_score_2, y_score_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular la diferencia entre TPR y FPR para cada umbral\n",
    "differences = tpr - fpr\n",
    "\n",
    "# Encontrar el índice del umbral que maximiza la diferencia\n",
    "optimal_threshold_index = np.argmax(differences)\n",
    "\n",
    "# Obtener el umbral óptimo\n",
    "optimal_threshold = thresholds[optimal_threshold_index]\n",
    "\n",
    "print(\"Optimal threshold:\", optimal_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr, fpr, roc_auc = experiment.to_test()\n",
    "\n",
    "with plt.style.context((\"seaborn-colorblind\")):\n",
    "    fig = plt.figure(figsize=(6, 4))\n",
    "    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate', fontsize='x-large')\n",
    "    plt.ylabel('True Positive Rate', fontsize='x-large')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    plt.tick_params(axis='both', which='major', labelsize='large')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
