{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "project_dir = os.path.join(os.getcwd(),'..')\n",
    "if project_dir not in sys.path:\n",
    "    sys.path.append(project_dir)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import config\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.path.join(config.BRATS_DATASET_PATH, 'BraTS2020_training_data/content/data/')\n",
    "\n",
    "# Create a list of all .h5 files in the directory\n",
    "h5_files = [f for f in os.listdir(directory) if f.endswith('.h5')]\n",
    "print(f\"Found {len(h5_files)} .h5 files:\\nExample file names:{h5_files[:3]}\")\n",
    "\n",
    "# Open the first .h5 file in the list to inspect its contents\n",
    "if h5_files:\n",
    "    file_path = os.path.join(directory, h5_files[25070])\n",
    "    with h5py.File(file_path, 'r') as file:\n",
    "        print(\"\\nKeys for each file:\", list(file.keys()))\n",
    "        for key in file.keys():\n",
    "            print(f\"\\nData type of {key}:\", type(file[key][()]))\n",
    "            print(f\"Shape of {key}:\", file[key].shape)\n",
    "            print(f\"Array dtype: {file[key].dtype}\")\n",
    "            print(f\"Array max val: {np.max(file[key])}\")\n",
    "            print(f\"Array min val: {np.min(file[key])}\")\n",
    "else:\n",
    "    print(\"No .h5 files found in the directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.facecolor'] = '#171717'\n",
    "plt.rcParams['text.color']       = '#DDDDDD'\n",
    "\n",
    "def display_image_channels(image, title='Image Channels'):\n",
    "    channel_names = ['T1-weighted (T1)', 'T1-weighted post contrast (T1c)', 'T2-weighted (T2)', 'Fluid Attenuated Inversion Recovery (FLAIR)']\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "    for idx, ax in enumerate(axes.flatten()):\n",
    "        channel_image = image[idx, :, :]  # Transpose the array to display the channel\n",
    "        ax.imshow(channel_image, cmap='magma')\n",
    "        ax.axis('off')\n",
    "        ax.set_title(channel_names[idx])\n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(title, fontsize=20, y=1.03)\n",
    "    plt.show()\n",
    "\n",
    "def display_mask_channels_as_rgb(mask, title='Mask Channels as RGB'):\n",
    "    channel_names = ['Necrotic (NEC)', 'Edema (ED)', 'Tumour (ET)']\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(9.75, 5))\n",
    "    for idx, ax in enumerate(axes):\n",
    "        rgb_mask = np.zeros((mask.shape[1], mask.shape[2], 3), dtype=np.uint8)\n",
    "        rgb_mask[..., idx] = mask[idx, :, :] * 255  # Transpose the array to display the channel\n",
    "        ax.imshow(rgb_mask)\n",
    "        ax.axis('off')\n",
    "        ax.set_title(channel_names[idx])\n",
    "    plt.suptitle(title, fontsize=20, y=0.93)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def overlay_masks_on_image(image, mask, title='Brain MRI with Tumour Masks Overlay'):\n",
    "    t1_image = image[0, :, :]  # Use the first channel of the image\n",
    "    t1_image_normalized = (t1_image - t1_image.min()) / (t1_image.max() - t1_image.min())\n",
    "\n",
    "    rgb_image = np.stack([t1_image_normalized] * 3, axis=-1)\n",
    "    color_mask = np.stack([mask[0, :, :], mask[1, :, :], mask[2, :, :]], axis=-1)\n",
    "    rgb_image = np.where(color_mask, color_mask, rgb_image)\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(rgb_image)\n",
    "    plt.title(title, fontsize=18, y=1.02)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "# Sample image to view\n",
    "sample_file_path = os.path.join(directory, h5_files[128])\n",
    "data = {}\n",
    "with h5py.File(sample_file_path, 'r') as file:\n",
    "    for key in file.keys():\n",
    "        data[key] = file[key][()]\n",
    "\n",
    "# Transpose the image and mask to have channels first\n",
    "image = data['image'].transpose(2, 0, 1)\n",
    "mask = data['mask'].transpose(2, 0, 1)\n",
    "\n",
    "# View images using plotting functions\n",
    "display_image_channels(image)\n",
    "display_mask_channels_as_rgb(mask)\n",
    "overlay_masks_on_image(image, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Since tumors mostly occur in the middle of the brain, we exclude the lowest 80 slices and the uppermost 26 slices.\" Reference: Diffusion Models for Medical Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular expression to extract the patient ID from the file name:\n",
    "# filename: volume_{id}_slice_{slice}.h5\n",
    "\n",
    "import re\n",
    "patient_slices = [re.search(r'volume_(\\d+)_slice_(\\d+)', f).groups() for f in h5_files]\n",
    "patient_slices\n",
    "\n",
    "filtered_patient_slices = list(filter(lambda p: 80 <= int(p[1]) < 128, patient_slices))\n",
    "len(filtered_patient_slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [f'volume_{p[0]}_slice_{p[1]}.h5' for p in filtered_patient_slices]\n",
    "filenames[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "labels = np.zeros(len(filenames))\n",
    "\n",
    "# Iterate through the files and check if the mask contains tumour\n",
    "for i in tqdm(range(len(filenames))):\n",
    "    sample_file_path = os.path.join(directory, filenames[i])\n",
    "    data = {}\n",
    "    with h5py.File(sample_file_path, 'r') as file:\n",
    "        for key in file.keys():\n",
    "            data[key] = file[key][()]\n",
    "\n",
    "    if len(np.unique(data['mask'])) > 1:\n",
    "        labels[i] = 1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({'Filename': filenames, 'Label': labels.astype(int)})\n",
    "df.to_csv('tumour_labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '/media/abian/Extreme SSD/WorkSpace/Dataset/BRATS/archive/'\n",
    "df.to_csv(os.path.join(dataset_path, 'tumour_labels.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_0 = 1209\n",
    "print(filenames[idx_0])\n",
    "sample_file_path = os.path.join(directory, filenames[idx_0])\n",
    "\n",
    "data_0 = {}\n",
    "with h5py.File(sample_file_path, 'r') as file:\n",
    "        for key in file.keys():\n",
    "            data_0[key] = file[key][()]\n",
    "\n",
    "idx_1 = 2512\n",
    "print(filenames[idx_1])\n",
    "sample_file_path = os.path.join(directory, filenames[idx_1])\n",
    "\n",
    "data_1 = {}\n",
    "with h5py.File(sample_file_path, 'r') as file:\n",
    "        for key in file.keys():\n",
    "            data_1[key] = file[key][()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(data_0['image'][:, :, 0].T, cmap='gray')\n",
    "plt.title('Patient 0')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(data_1['image'][:, :, 0].T, cmap='gray')\n",
    "plt.title('Patient 1')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_0['image'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(config.BRATS_DATASET_PATH, 'tumor_labels.csv'))\n",
    "filenames = df['Filename'].values\n",
    "\n",
    "no_tumor_filenames = df[df['Label'] == 0]['Filename'].values\n",
    "\n",
    "sample_file_path = os.path.join(directory, no_tumor_filenames[10])\n",
    "data = {}\n",
    "with h5py.File(sample_file_path, 'r') as file:\n",
    "        for key in file.keys():\n",
    "            data[key] = file[key][()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.bincount(df['Label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "for i in range(4):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    plt.imshow(data['image'][:, :, i].T, cmap='gray')\n",
    "    plt.title(f'Channel {i}')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class AnoamlyBrainTumor(Dataset):\n",
    "    ''' \n",
    "        BRATS 2020 dataset adapted for anomaly detection. Since tumors mostly occur in the middle of the brain,\n",
    "        it is excluded the lowest 80 slices and the uppermost 26 slices from the dataset.\n",
    "    '''\n",
    "    def __init__(self, dataset_path:str, transform=None, on_memory=False) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.on_memory = on_memory\n",
    "        self.dataset_path = os.path.join(dataset_path, 'BraTS2020_training_data/content/data/')\n",
    "        self.transform = transform\n",
    "\n",
    "        self.df = pd.read_csv(os.path.join(dataset_path, 'tumor_labels.csv'))\n",
    "        self.filenames = list(map(lambda x: os.path.join(self.dataset_path, x), self.df['Filename'].values))\n",
    "        self.labels = self.df['Label'].values\n",
    "\n",
    "        if self.on_memory:\n",
    "            self.data = self.__loaddata__(self.filenames)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.filenames)\n",
    "    \n",
    "    def __getitem__(self, idx:int) -> torch.Tensor:\n",
    "        if self.on_memory: \n",
    "            data = self.data[idx]\n",
    "        else:\n",
    "            data = self.__readfile__(self.filenames[idx])\n",
    "\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return data, self.labels[idx]\n",
    "\n",
    "    def __loaddata__(self, filenames:list) -> np.ndarray:\n",
    "        shape = (len(filenames), 240, 240, 4) # BRATS 2020 dataset shape\n",
    "        dataset = np.zeros(shape, dtype=np.float32)\n",
    "        for idx, filename in enumerate(filenames):\n",
    "            with h5py.File(filename, 'r') as file:\n",
    "                dataset[idx] = file['image'][()].astype(np.float32)\n",
    "                file.close()\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def __readfile__(self, filename:str)-> np.ndarray:\n",
    "        with h5py.File(filename, 'r') as file:\n",
    "            data = file['image'][()].astype(np.float32)\n",
    "            file.close()\n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_brats_tensor(tensor: torch.Tensor) -> torch.Tensor:\n",
    "    tensor = tensor - torch.min(tensor.view(4,-1), dim=1).values.reshape(4,1,1)\n",
    "    return tensor / torch.max(tensor.view(4,-1), dim=1).values.reshape(4,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: normalize_brats_tensor(x)),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "dataset = AnoamlyBrainTumor(config.BRATS_DATASET_PATH, transform=transform, on_memory=True)\n",
    "# loader = DataLoader(AnoamlyBrainTumor(dataset_path), batch_size=128, shuffle=True)\n",
    "# for data, label in loader:\n",
    "#     print(data.shape, label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "no_tumor_idx = np.where(dataset.labels == 0)[0]\n",
    "tumor_idx = np.where(dataset.labels == 1)[0]\n",
    "\n",
    "# Select a subset of the dataset for test\n",
    "test_set = Subset(dataset, np.concatenate([no_tumor_idx[:100], tumor_idx[:100]]))\n",
    "print(f\"Number of samples in test subset: {len(test_set)}\")\n",
    "\n",
    "train_set = Subset(dataset, np.concatenate([no_tumor_idx[100:], tumor_idx[100:100+512]]))\n",
    "print(f\"Number of samples in training subset: {len(train_set)}\")\n",
    "\n",
    "# Check the distribution of labels in the training set\n",
    "np.bincount([dataset.labels[i] for i in train_set.indices])\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from VAE.AnomalyDetector import AnomalyDetector\n",
    "from VAE.utils import SGVBL, cosine_scheduler\n",
    "\n",
    "class VAEModel(nn.Module):\n",
    "    def __init__(self, input_size, latent_space):\n",
    "        super(VAEModel, self).__init__()\n",
    "        conv_out_size = input_size // (2*2*2)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(4, 16, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Dropout2d(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3, stride=2, padding=1),\n",
    "            nn.Conv2d(16, 32, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Dropout2d(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3, stride=2, padding=1),\n",
    "            nn.Conv2d(32, 64, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Dropout2d(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3, stride=2, padding=1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*(conv_out_size**2), 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.botleneck = AnomalyDetector(128, latent_space)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_space, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 64*(conv_out_size**2)),\n",
    "            nn.BatchNorm1d(64*(conv_out_size**2)),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(1, (64, conv_out_size, conv_out_size)),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(64, 32, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Dropout2d(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(32, 16, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Dropout2d(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(16, 4, 3, stride=1, padding=1),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
    "        x = self.encoder(x)\n",
    "        x = self.botleneck(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "model = VAEModel(240, 10)\n",
    "from torch.nn.functional import mse_loss\n",
    "sgvbl = SGVBL(model, len(train_set), mle=mse_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "model.train()\n",
    "n_epochs = 100\n",
    "from tqdm import tqdm\n",
    "# kl_weight = 0.02\n",
    "\n",
    "epoch_iterator = tqdm(\n",
    "        range(n_epochs),\n",
    "        leave=True,\n",
    "        unit=\"epoch\",\n",
    "        postfix={\"tls\": \"%.4f\" % -1},\n",
    "    )\n",
    "\n",
    "kl_weight = 1.5*cosine_scheduler(n_epochs)\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "for epoch in epoch_iterator:\n",
    "    epoch_loss = 0.\n",
    "    # kl_weight = min(kl_weight+0.012, .9)\n",
    "    for x, y in train_loader:\n",
    "        # check if there are a target with 1\n",
    "        # if torch.any(y == 1):\n",
    "            # print(\"Anomaly detected\")\n",
    "            # break\n",
    "        x = x.to(device) # GPU\n",
    "        opt.zero_grad()\n",
    "        x_hat = torch.tanh(model(x))\n",
    "        loss = sgvbl(x, x_hat, y, kl_weight[epoch])\n",
    "        # loss = sgvbl(x, x_hat, y, 1)\n",
    "        epoch_loss += loss.detach().item()\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    \n",
    "    epoch_iterator.set_postfix(tls=\"%.3f\" % (epoch_loss/len(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_hat = torch.tanh(model(x))[0,0]\n",
    "\n",
    "plt.imshow(x_hat.cpu().detach().numpy(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def plot_reconstructed(autoencoder, r0=(-10, 10), r1=(-10, 10), n=12):\n",
    "    w = 240\n",
    "    img = np.zeros((n*w, n*w))\n",
    "    for i, y in enumerate(np.linspace(*r1, n)):\n",
    "        for j, x in enumerate(np.linspace(*r0, n)):\n",
    "            z = torch.Tensor([[x, y]]).to(device)\n",
    "            x_hat = torch.tanh(autoencoder.decoder(z))\n",
    "            x_hat = x_hat.reshape(4, 240, 240)[1].to('cpu').detach().numpy()\n",
    "            img[(n-1-i)*w:(n-1-i+1)*w, j*w:(j+1)*w] = x_hat\n",
    "    \n",
    "    plt.xlabel('$\\mathcal{N}(0, \\sigma_1)$', fontsize='x-large')\n",
    "    plt.ylabel('$\\mathcal{N}(0, \\sigma_2)$', fontsize='x-large')\n",
    "    plt.imshow(img, extent=[*r0, *r1], cmap='viridis')\n",
    "\n",
    "model.eval()\n",
    "plot_reconstructed(model, r0=(-6, 6), r1=(-6, 6), n=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = dataset[0]\n",
    "model.encoder[0:11](x.unsqueeze(0).to(device)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "x_0, y_0 = test_set[25]\n",
    "x_1, y_1 = test_set[125]\n",
    "\n",
    "# x_0 = dataset[no_tumor_idx[0]][0].unsqueeze(0).to(device)\n",
    "# x_1 = dataset[tumor_idx[100]][0].unsqueeze(0).to(device)\n",
    "\n",
    "x = torch.stack([x_0, x_1]).to(device)\n",
    "x_hat = torch.tanh(model(x))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.imshow(x[0,0].cpu().detach().numpy(), cmap='gray')\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.imshow(x_hat[0,0].cpu().detach().numpy(), cmap='gray')\n",
    "plt.title('Reconstructed Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.imshow(x[1,0].cpu().detach().numpy(), cmap='gray')\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.imshow(x_hat[1,0].cpu().detach().numpy(), cmap='gray')\n",
    "plt.title('Reconstructed Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "model.botleneck.mu, model.botleneck.sigma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = DataLoader(test_set, batch_size=200, shuffle=False)\n",
    "x, y = next(iter(test))\n",
    "\n",
    "model.eval()\n",
    "x_hat = torch.tanh(model(x.to(device)))\n",
    "\n",
    "mu, sigma = model.botleneck.mu, model.botleneck.sigma\n",
    "mu = mu.cpu().detach().numpy()\n",
    "sigma = sigma.cpu().detach().numpy()\n",
    "\n",
    "print('Normal: {}, {}'.format(mu[:100].mean(), sigma[:100].mean()))\n",
    "print('Anomaly: {}, {}'.format(mu[100:].mean(), sigma[100:].mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AE Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from itertools import islice\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "## General\n",
    "def slide(iterable, size):\n",
    "    '''\n",
    "        Iterate through iterable using a sliding window of several elements.\n",
    "        Important: It is a generator!.\n",
    "        \n",
    "        Creates an iterable where each element is a tuple of `size`\n",
    "        consecutive elements from `iterable`, advancing by 1 element each\n",
    "        time. For example:\n",
    "        >>> list(sliding_window_iter([1, 2, 3, 4], 2))\n",
    "        [(1, 2), (2, 3), (3, 4)]\n",
    "        \n",
    "        source: https://codereview.stackexchange.com/questions/239352/sliding-window-iteration-in-python\n",
    "    '''\n",
    "    iterable = iter(iterable)\n",
    "    window = deque(islice(iterable, size), maxlen=size)\n",
    "    for item in iterable:\n",
    "        yield tuple(window)\n",
    "        window.append(item)\n",
    "    if window:  \n",
    "        # needed because if iterable was already empty before the `for`,\n",
    "        # then the window would be yielded twice.\n",
    "        yield tuple(window)\n",
    "\n",
    "class SkipConnectionSequential(nn.Sequential):\n",
    "    '''\n",
    "        Sequential module return the output of the last layer and the intermediate\n",
    "        for the skip connections.\n",
    "    '''\n",
    "    def __init__(self, *args) -> None:\n",
    "        super(SkipConnectionSequential, self).__init__(*args)\n",
    "\n",
    "    def forward(self, x) -> tuple:\n",
    "        ''' \n",
    "            Returns\n",
    "            -------\n",
    "            x: torch.Tensor,\n",
    "                The output of the encode path.\n",
    "\n",
    "            sk: list,\n",
    "                A list of the intermediate outputs of the encode path, representing\n",
    "                the skip connections. This list is ordered by the depth of the\n",
    "                skip connections, where the first element is the deepest and the\n",
    "                last element is the shallowest.\n",
    "        '''\n",
    "\n",
    "        sk = []\n",
    "        for module in self:\n",
    "            x = module(x)\n",
    "            sk.append(x.clone())\n",
    "        \n",
    "        sk.reverse()\n",
    "        return x, sk\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, channels:list, skip_connection=False) -> None:\n",
    "        super(Encoder, self).__init__()\n",
    "        encode_path = []\n",
    "        for _channels in slide(channels, 2):\n",
    "            encode_path.append(self.__encode_module__(*_channels))\n",
    "        \n",
    "        self.encode_path = nn.Sequential(*encode_path) if not skip_connection else SkipConnectionSequential(*encode_path)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:          \n",
    "        return self.encode_path(x)\n",
    "    \n",
    "    def __encode_module__(self, in_channels, out_channels):\n",
    "        '''\n",
    "            The encoding path is designed to reduce the spatial dimensions of \n",
    "            the input tensor by a factor of 2. For example, if the input image\n",
    "            is 240x240 pixels, the output image will be downscaled to 120x120 pixels.\n",
    "        '''\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.Dropout2d(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3, stride=2, padding=1),\n",
    "        )\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, channels:list, skip_connection=False) -> None:\n",
    "        super(Decoder, self).__init__()\n",
    "        decode_path = []\n",
    "        for idx, _channels in enumerate(slide(channels, 2)):\n",
    "            decode_path.append(self.__decode_module__(*_channels, activation=True if idx < len(channels)-2 else False, skip_connection=skip_connection))\n",
    "        \n",
    "        self.decode_path = nn.Sequential(*decode_path) \n",
    "        self.skip_connection = skip_connection\n",
    "\n",
    "    def forward(self, x: torch.Tensor, **kwargs) -> torch.Tensor:\n",
    "        if self.skip_connection:\n",
    "            return self.__forward_skip_connection__(x, **kwargs)\n",
    "\n",
    "        return self.decode_path(x)\n",
    "    \n",
    "    def __forward_skip_connection__(self, x: torch.Tensor, skip:list) -> torch.Tensor:\n",
    "        assert len(skip) == len(self.decode_path) - 1 # Excluding the output layer\n",
    "        for idx, layer in enumerate(self.decode_path[:-1]):\n",
    "            up, *layers = layer\n",
    "            x = up(x)\n",
    "            x = torch.cat((x, skip[idx]), dim=1)\n",
    "            print(x.shape)\n",
    "            for layer in layers:\n",
    "                x = layer(x)                   \n",
    "\n",
    "        return self.decode_path[-1](x)\n",
    "    \n",
    "      \n",
    "    def __decode_module__(self, in_channels, out_channels, activation=True, skip_connection=False):\n",
    "        '''\n",
    "            The decoding path is designed to increase the spatial dimensions of \n",
    "            the input tensor by a factor of 2. For example, if the input image\n",
    "            is 120x120 pixels, the output image will be upscaled to 240x240 pixels.\n",
    "        '''\n",
    "        return nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.Conv2d(in_channels + out_channels if skip_connection and activation else in_channels, out_channels, 3, stride=1, padding=1),\n",
    "            *(nn.BatchNorm2d(out_channels),nn.Dropout2d(0.2), nn.ReLU()) if activation else (nn.Identity(),)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Encoder([4, 16, 32], skip_connection=True)\n",
    "x = torch.randn(1, 4, 240, 240)\n",
    "y, enc_sk = test(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_sk[0].shape, enc_sk[1].shape, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Decoder([4, 16, 32, 8][::-1], skip_connection=True)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 8, 30, 30)\n",
    "sk = enc_sk\n",
    "test(x, skip=sk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from VAE.AnomalyDetector import AnomalyDetector\n",
    "from VAE.utils import SGVBL, cosine_scheduler\n",
    "\n",
    "class VAEModel(nn.Module):\n",
    "    def __init__(self, input_size, latent_space):\n",
    "        super(VAEModel, self).__init__()\n",
    "        conv_out_size = input_size // (2*2*2)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(4, 16, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Dropout2d(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3, stride=2, padding=1),\n",
    "            nn.Conv2d(16, 32, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Dropout2d(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3, stride=2, padding=1),\n",
    "            nn.Conv2d(32, 64, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Dropout2d(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3, stride=2, paddishape 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.botleneck = AnomalyDetector(128, latent_space)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_space, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 64*(conv_out_size**2)),\n",
    "            nn.BatchNorm1d(64*(conv_out_size**2)),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(1, (64, conv_out_size, conv_out_size)),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(64, 32, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Dropout2d(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(32, 16, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Dropout2d(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(16, 4, 3, stride=1, padding=1),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
    "        x = self.encoder(x)\n",
    "        x = self.botleneck(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "model = VAEModel(240, 10)\n",
    "from torch.nn.functional import mse_loss\n",
    "sgvbl = SGVBL(model, len(train_set), mle=mse_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
