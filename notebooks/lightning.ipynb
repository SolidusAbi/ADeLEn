{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "project_dir = os.path.join(os.getcwd(),'..')\n",
    "if project_dir not in sys.path:\n",
    "    sys.path.append(project_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch import nn\n",
    "# from VAE.nn import AnomalyDetector\n",
    "# from functools import reduce\n",
    "\n",
    "# class StethoscopeEncoder(nn.Module):\n",
    "#     def __init__(self, dropout=True) -> None:\n",
    "#         super(StethoscopeEncoder, self).__init__()\n",
    "\n",
    "#         self.encode = nn.Sequential(\n",
    "#             nn.Sequential(\n",
    "#                 nn.Conv2d(1, 16, 3, padding=1),\n",
    "#                 nn.BatchNorm2d(16, affine=True),\n",
    "#                 *(nn.Dropout2d(0.2), nn.ReLU()) if dropout else (nn.ReLU(),),\n",
    "#                 nn.MaxPool2d(3, stride=2, padding=1)\n",
    "#             ),\n",
    "#             nn.Sequential(\n",
    "#                 nn.Conv2d(16, 32, (3, 5), padding=1), # (124,10) -> (122, 8)\n",
    "#                 nn.BatchNorm2d(32, affine=True),\n",
    "#                 *(nn.Dropout2d(0.2), nn.ReLU()) if dropout else (nn.ReLU(),),\n",
    "#                 nn.MaxPool2d(3, stride=2, padding=1)\n",
    "#             ),\n",
    "#             nn.Sequential(\n",
    "#                 nn.Conv2d(32, 36, (7, 3), padding=0), # (122, 8) -> (56, 2)\n",
    "#                 nn.BatchNorm2d(36, affine=True),\n",
    "#                 *(nn.Dropout2d(0.2), nn.ReLU()) if dropout else (nn.ReLU(),),\n",
    "#                 nn.MaxPool2d(3, stride=2, padding=1)\n",
    "#             )\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.encode(x)\n",
    "    \n",
    "# class StethoscopeDecoder(nn.Module):\n",
    "#     def __init__(self) -> None:\n",
    "#         super(StethoscopeDecoder, self).__init__()\n",
    "\n",
    "#         self.decode = nn.Sequential(\n",
    "#             nn.Sequential(\n",
    "#                 nn.Upsample(scale_factor=2),\n",
    "#                 nn.ConvTranspose2d(36, 32, (7, 3), padding=0),\n",
    "#                 nn.BatchNorm2d(32, affine=True),\n",
    "#                 nn.ReLU()\n",
    "#             ),\n",
    "#             nn.Sequential(\n",
    "#                 nn.Upsample(scale_factor=2),\n",
    "#                 nn.ConvTranspose2d(32, 16, (3, 5), padding=1),\n",
    "#                 nn.BatchNorm2d(16, affine=True),\n",
    "#                 nn.ReLU()\n",
    "#             ),\n",
    "#             nn.Sequential(\n",
    "#                 nn.Upsample(scale_factor=2),\n",
    "#                 nn.Conv2d(16, 1, 3, padding=1)\n",
    "#             )\n",
    "#         )\n",
    "\n",
    "#     def forward(self, z):\n",
    "#         return self.decode(z)\n",
    "    \n",
    "\n",
    "# class Model(nn.Module):\n",
    "#     def __init__(self, bottleneck=2, sigma=3) -> None:\n",
    "#         super(Model, self).__init__()\n",
    "\n",
    "#         self.encoder = self.__encode_path__()\n",
    "#         self.bottleneck = AnomalyDetector(\n",
    "#             in_features=128, out_features=bottleneck, sigma_anomaly=sigma)\n",
    "#         self.decoder = self.__decode_path__()\n",
    "        \n",
    "\n",
    "#     def forward(self, x):\n",
    "#         z = self.encoder(x)\n",
    "#         z = self.bottleneck(z)\n",
    "#         x_ = self.decoder(z)\n",
    "#         return x_\n",
    "    \n",
    "#     def __encode_path__(self):\n",
    "#         conv_encoder = StethoscopeEncoder()\n",
    "#         encoded_size = 36 * 28 * 1 # 36 channels, 28x1 image\n",
    "#         linear_encoder = nn.Sequential(\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.Linear(encoded_size, 128),\n",
    "#             nn.BatchNorm1d(128, affine=True),\n",
    "#             nn.ReLU(),\n",
    "#         )\n",
    "\n",
    "#         return nn.Sequential(\n",
    "#             conv_encoder,\n",
    "#             nn.Flatten(),\n",
    "#             linear_encoder\n",
    "#         )\n",
    "    \n",
    "#     def __decode_path__(self):\n",
    "#         encoded_shape = (36, 28, 1) # 36 channels, 28x1 image\n",
    "#         encoded_size = reduce(lambda x, y: x*y, encoded_shape)\n",
    "#         conv_decoder = StethoscopeDecoder()\n",
    "#         linear_decoder = nn.Sequential(\n",
    "#             nn.Linear(2, 128),\n",
    "#             nn.BatchNorm1d(128, affine=True),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(128, 36 * 28 * 1),\n",
    "#             nn.BatchNorm1d(36 * 28 * 1, affine=True),\n",
    "#             nn.ReLU(),\n",
    "#         )\n",
    "\n",
    "#         return nn.Sequential(\n",
    "#             linear_decoder,\n",
    "#             nn.Unflatten(1, encoded_shape),\n",
    "#             conv_decoder\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.nn.functional import mse_loss\n",
    "from VAE.loss import SGVBL\n",
    "import numpy as np\n",
    "\n",
    "def weights(dataset):\n",
    "    _, y = zip(*dataset)\n",
    "    y = torch.tensor(y)\n",
    "\n",
    "    count = torch.bincount(y)\n",
    "    weights = 1. / np.array(count)\n",
    "    weights /= weights.sum()\n",
    "\n",
    "    return weights[y]\n",
    "\n",
    "def train(model, train_dataset, batch_size, n_epochs, lr=1e-3, kl_weight=1, weighted_sampler=False, **kwargs):\n",
    "    if weighted_sampler:\n",
    "        sampler = torch.utils.data.WeightedRandomSampler(weights(train_dataset), len(train_dataset), replacement=True)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler)\n",
    "    else:\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    epoch_iterator = tqdm(\n",
    "            range(n_epochs),\n",
    "            leave=False,\n",
    "            unit=\"epoch\",\n",
    "            postfix={\"tls\": \"%.4f\" % -1},\n",
    "        )\n",
    "    \n",
    "    opt = Adam(model.parameters(), lr=lr)\n",
    "    sgvbl = SGVBL(model, len(train_dataset), mle=mse_loss)\n",
    "    for _ in epoch_iterator:\n",
    "        epoch_loss = 0.\n",
    "        for x, y in train_loader:\n",
    "            x = x.to(device) \n",
    "            opt.zero_grad()\n",
    "            x_hat = torch.tanh(model(x))\n",
    "            loss = sgvbl(x, x_hat, y, kl_weight)\n",
    "            epoch_loss += loss.detach().item()\n",
    "\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        epoch_iterator.set_postfix(tls=\"%.3f\" % (epoch_loss/len(train_loader)))\n",
    "\n",
    "    return model.eval().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from VAE.loss import SGVBL\n",
    "from torch.nn.functional import mse_loss\n",
    "from ADeLEn.model import ADeLEn\n",
    "\n",
    "class LightningModel(pl.LightningModule):\n",
    "    def __init__(self, model: ADeLEn) -> None:\n",
    "        super(LightningModel, self).__init__()\n",
    "        self.model = model\n",
    "        self.sgvbl = SGVBL(self.model, 1, mle=mse_loss)\n",
    "\n",
    "    def training_step(self, batch, batch_idx) -> torch.Tensor:\n",
    "        x, y = batch\n",
    "        x_hat = torch.tanh(self.model(x))\n",
    "        loss = self.sgvbl(x, x_hat, y, kl_weight=1)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self) -> torch.optim.Optimizer:\n",
    "        return torch.optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "    \n",
    "    def test_step(self, batch, batch_idx) -> torch.Tensor:\n",
    "        x, y = batch\n",
    "        x_hat = torch.tanh(self.model(x))\n",
    "        loss = self.sgvbl(x, x_hat, y, kl_weight=1)\n",
    "        return loss\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        '''\n",
    "            score samples per label\n",
    "\n",
    "            Parameters:\n",
    "            -----------\n",
    "            X : torch.Tensor\n",
    "                input data\n",
    "            y : torch.Tensor\n",
    "                labels  \n",
    "        '''\n",
    "        score = self.model.score_samples(X)\n",
    "        return [ score[torch.argwhere(y==i).squeeze()] for i in torch.unique(y) ]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import AnomalyMNIST\n",
    "from torchvision.transforms import Normalize,ToTensor, Compose\n",
    "\n",
    "seed = 42\n",
    "transform = Compose([ToTensor(), Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# dataset = AnomalyMNIST('data/', download=True, transform=transform, n_known_anomalies=512, pollution=0.25, seed=seed)\n",
    "dataset = AnomalyMNIST('data/', download=True, transform=transform, n_normal_samples=2000, known_anomalies=0.05, pollution=0.05, seed=seed)\n",
    "print(dataset)\n",
    "\n",
    "fig = dataset.montage(5, 5, seed)\n",
    "# fig.savefig('mnist_montage.png', dpi=300, bbox_inches='tight')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = zip(*dataset)\n",
    "X = torch.stack(X, dim=0)\n",
    "y = torch.tensor(y)\n",
    "\n",
    "def test(y:torch.Tensor):\n",
    "     torch.argwhere(y == 1).squeeze()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape, torch.argwhere(y == 1).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i.item() for i in torch.argwhere(y == 1).squeeze()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Include the test set configuration in the AnoamlyMNIST class\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "test_dataset_full = MNIST('data/', train = False, download = True, transform = transform)\n",
    "normal_idx = torch.where((test_dataset_full.targets == 1))[0]\n",
    "anomaly_idx = torch.where((test_dataset_full.targets == 7))[0]\n",
    "idx = torch.cat([normal_idx[:256], anomaly_idx[:256]]) # 512 samples!\n",
    "\n",
    "test_dataset_full.targets = torch.ones_like(test_dataset_full.targets) * -1\n",
    "test_dataset_full.targets[normal_idx] = 0\n",
    "test_dataset_full.targets[anomaly_idx] = 1\n",
    "\n",
    "x_test_set = Subset(test_dataset_full, idx)\n",
    "test_loader =  DataLoader(x_test_set, 512, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ADeLEn.model import ADeLEn\n",
    "from VAE.loss import SGVBL\n",
    "\n",
    "adelen = ADeLEn((28, 28), [1, 32, 48], [1024, 256, 32], bottleneck=2, skip_connection=False)\n",
    "model = LightningModel(adelen)\n",
    "\n",
    "# # model = ADeLEn((28, 28), [1, 12, 32], [1024, 512, 128, 2], skip_connection=False)\n",
    "# d = 2\n",
    "# model = ADeLEn((28, 28), [1, 32, 48], [1024, 256, 32], bottleneck=d, skip_connection=False)\n",
    "# from torch.nn.functional import mse_loss\n",
    "# sgvbl = SGVBL(model, len(dataset), mle=mse_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = train(model, dataset, 128, 50, 1e-3, 1, weighted_sampler=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(max_epochs=50, log_every_n_steps=0)\n",
    "trainer.fit(model, train_loader)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = zip(*dataset)\n",
    "X = torch.stack(X)\n",
    "y = torch.tensor(y)\n",
    "model.eval()\n",
    "score = model.model.score_samples(X)\n",
    "score.shape\n",
    "\n",
    "scores = [\n",
    "    score[torch.argwhere(y==i).squeeze()] for i in torch.unique(y)\n",
    "]\n",
    "\n",
    "for _score in scores:\n",
    "    plt.hist(_score[:100], bins=10, alpha=0.5)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.score(X, y)\n",
    "\n",
    "for _score in scores:\n",
    "    plt.hist(_score[:100], bins=10, alpha=0.5)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores[0].shape, scores[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = zip(*dataset)\n",
    "X = torch.stack(X)\n",
    "y = torch.tensor(y)\n",
    "\n",
    "normal = X[y == 0]\n",
    "model.eval()\n",
    "_ = model.model(normal).detach()\n",
    "model.model.bottleneck.sigma.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def plot_latent(model, data, num_batches=100):\n",
    "    fig = plt.figure(figsize=(5,5))\n",
    "    with torch.no_grad():\n",
    "        for i, (x, y) in enumerate(data):\n",
    "            # x = x.to(device)\n",
    "            z = model.bottleneck(model.encode_path(x))\n",
    "            z = z.cpu().detach().numpy()\n",
    "            y = y.cpu().detach().numpy()\n",
    "            anomalies = np.where(y == 1)\n",
    "            normal = np.where(y == 0)\n",
    "            if i == 0:\n",
    "                plt.scatter(z[normal, 0], z[normal, 1], c='r', alpha=.7, label='normal')\n",
    "                plt.scatter(z[anomalies, 0], z[anomalies, 1], c='b', alpha=.7, label='anomalies')\n",
    "            else:\n",
    "                plt.scatter(z[normal, 0], z[normal, 1], c='r',alpha=.7)\n",
    "                plt.scatter(z[anomalies, 0], z[anomalies, 1], c='b',alpha=.7)\n",
    "           \n",
    "            if i > num_batches:\n",
    "                plt.legend()\n",
    "                return\n",
    "    plt.legend()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_latent(model.model, test_loader, num_batches=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(test_loader))\n",
    "x_0 = x[torch.argwhere(y==0).squeeze()[150]].unsqueeze(0)\n",
    "x_1 = x[torch.argwhere(y==1).squeeze()[152]].unsqueeze(0)\n",
    "\n",
    "print(x_0.shape)\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(x_0[0,0])\n",
    "plt.axis('off')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(x_1[0,0])\n",
    "plt.axis('off')\n",
    "model.eval()\n",
    "z_0, z_1 = [], []\n",
    "for i in range(100):\n",
    "    z_0.append(model.model.bottleneck(model.model.encode_path(x_0)).detach().cpu())\n",
    "    z_1.append(model.model.bottleneck(model.model.encode_path(x_1)).detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(torch.cat(z_0)[:,0], torch.cat(z_0)[:,1], c='r')\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(torch.cat(z_1)[:,0], torch.cat(z_1)[:,1], c='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.cat([x_0, x_1])\n",
    "print(x.shape)\n",
    "\n",
    "with torch.no_grad():\n",
    "    x_hat = model.model(x)\n",
    "\n",
    "model.model.bottleneck.sigma.detach(), model.model.bottleneck.mu.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(test_loader))\n",
    "x_0 = x[torch.argwhere(y==0).squeeze()[:200]]\n",
    "x_1 = x[torch.argwhere(y==1).squeeze()[:200]]\n",
    "\n",
    "x = torch.cat([x_0, x_1])\n",
    "with torch.no_grad():\n",
    "    x_hat = model.model(x)\n",
    "\n",
    "model.model.bottleneck.sigma[:200].detach().mean(), model.model.bottleneck.sigma[200:400].detach().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the idx of the highest 3 sigma\n",
    "idx = torch.argsort(model.model.bottleneck.sigma[:200].mean(axis=1), descending=True)\n",
    "model.model.bottleneck.sigma[idx[:3]]\n",
    "\n",
    "to_show = x[idx[:3]]\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "for i in range(3):\n",
    "    plt.subplot(1,3,i+1)\n",
    "    plt.imshow(to_show[i,0])\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def plot_reconstructed(model, r0=(-6, 6), r1=(-6, 6), n=12):\n",
    "    model.eval()\n",
    "    w = 28\n",
    "    img = np.zeros((n*w, n*w))\n",
    "\n",
    "    fig = plt.figure(figsize=(5, 5))\n",
    "\n",
    "    for i, y in enumerate(np.linspace(*r1, n)):\n",
    "        for j, x in enumerate(np.linspace(*r0, n)):\n",
    "            z = torch.Tensor([[x, y]])\n",
    "            x_hat = torch.tanh(model.decode_path(z)) # ADeLEn\n",
    "            x_hat = x_hat.reshape(w, w).to('cpu').detach().numpy()\n",
    "            img[(n-1-i)*w:(n-1-i+1)*w, j*w:(j+1)*w] = x_hat\n",
    "    \n",
    "    plt.xlabel(r'$\\mathcal{N}(0, \\sigma_1)$', fontsize='xx-large')\n",
    "    plt.ylabel(r'$\\mathcal{N}(0, \\sigma_2)$', fontsize='xx-large')\n",
    "    plt.tick_params(axis='both', which='major', labelsize='x-large')\n",
    "    plt.imshow(img, extent=[*r0, *r1], cmap='viridis')\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_reconstructed(model.model, r0=(-6, 6), r1=(-6, 6), n=13)\n",
    "# plt.savefig('figures/reconstructed.pdf', bbox_inches='tight')\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision.transforms import Normalize\n",
    "# from dataset import StethoscopeAnomalyDataset\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# normalize = Normalize(mean=(0.01,), std=(0.06,))\n",
    "# dataset = StethoscopeAnomalyDataset(config.STETHOSCOPE_DATASET_PATH, transform=normalize)\n",
    "# train_loader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from model import Model\n",
    "\n",
    "# model = LightningModel(Model(2, 3))\n",
    "# trainer = pl.Trainer(max_epochs=100)\n",
    "# trainer.fit(model, train_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ADeLEn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
