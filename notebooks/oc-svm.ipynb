{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "project_dir = os.path.join(os.getcwd(),'..')\n",
    "if project_dir not in sys.path:\n",
    "    sys.path.append(project_dir)\n",
    "\n",
    "from experiments.MNIST import ExperimentSVM\n",
    "from experiments.utils import generate_roc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "def generate_roc_df(roc_list:list) -> pd.DataFrame:\n",
    "    ''' \n",
    "        Create a DataFrame from a list of roc curves\n",
    "        Args:\n",
    "        -----\n",
    "            roc_list: list\n",
    "                List of N roc curves where N is the number of iterations. It is a \n",
    "                list of tuples of the form (fpr, tpr), where fpr is the false positive\n",
    "                rate and tpr is the true positive rate.\n",
    "        Returns:\n",
    "        --------\n",
    "            roc_df: pd.DataFrame\n",
    "                DataFrame with multiindex\n",
    "    '''\n",
    "    index_names = [\n",
    "        list(map(lambda x: 'It {}'.format(x), np.repeat(np.arange(len(roc_list)), 2) + 1 )),\n",
    "        ['FPR', 'TPR']*len(roc_list)\n",
    "    ]\n",
    "        \n",
    "    tuples = list(zip(*index_names))\n",
    "    index = pd.MultiIndex.from_tuples(tuples)\n",
    "    roc_df = pd.DataFrame(chain.from_iterable(roc_list), index=index)\n",
    "    return roc_df\n",
    "\n",
    "def generate_multi_df(data:list, index_names:list) -> pd.DataFrame:\n",
    "    index_names = [\n",
    "        list(map(lambda x: 'It {}'.format(x), np.repeat(np.arange(len(data)), len(index_names)) + 1 )),\n",
    "        index_names*len(data)\n",
    "    ]\n",
    "\n",
    "    tuples = list(zip(*index_names))\n",
    "    index = pd.MultiIndex.from_tuples(tuples)\n",
    "    return pd.DataFrame(chain.from_iterable(data), index=index)\n",
    "\n",
    "def save_result(roc:list, scores:list, metrics:np.ndarray, config:dict) -> tuple:\n",
    "    '''\n",
    "        Save the results of the experiment\n",
    "        Args:\n",
    "        -----\n",
    "            roc: list\n",
    "                List of roc curves\n",
    "            auc: list\n",
    "                List of AUC scores\n",
    "            config: dict\n",
    "                Configuration of the experiment\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "            roc_df: pd.DataFrame\n",
    "                ROC Cuve dataFrame with multiindex, considering the iterations.\n",
    "            auc_df: pd.DataFrame\n",
    "                DataFrame with the AUC scores\n",
    "    '''\n",
    "\n",
    "    roc_df = generate_multi_df(roc, ['FPR', 'TPR']).T\n",
    "    scores_df = generate_multi_df(scores, ['Normal', 'Anomaly']).T\n",
    "    metrics_df = pd.DataFrame(metrics, columns=['Accuracy', 'Precision', 'Recall', 'F1', 'AUC'])\n",
    "\n",
    "    roc_df.to_pickle(os.path.join(config['save_result_dir'], 'roc.pkl'))\n",
    "    scores_df.to_pickle(os.path.join(config['save_result_dir'], 'sample_score.pkl'))\n",
    "    metrics_df.to_csv(os.path.join(config['save_result_dir'], 'metrics.csv'))\n",
    "    \n",
    "    return roc_df, scores_df, metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 25\n",
    "seed = 2*np.arange(n_iter, dtype=int) + 42\n",
    "\n",
    "pollution_exp = [0, .05, .1, .2]\n",
    "for pollution in pollution_exp:\n",
    "    print(f'Exp. {pollution=}')\n",
    "    iterator = tqdm(\n",
    "                range(n_iter),\n",
    "                leave=True,\n",
    "                unit=\"It.\",\n",
    "                postfix={\"AUC\": \"%.3f\" % -1},\n",
    "            )\n",
    "\n",
    "    roc, scores = [], []\n",
    "    metrics = np.empty((n_iter, 5)) # acc, prec, rec, f1, auc\n",
    "\n",
    "    for it in iterator:\n",
    "        exp = ExperimentSVM(known_anomalies=.1, pollution=pollution, seed=int(seed[it]))\n",
    "        if it == 0:\n",
    "            config = exp.config()\n",
    "            exp.save_config()\n",
    "        \n",
    "        \n",
    "        auc_score = exp.run(verbose=0)\n",
    "        iterator.set_postfix({\"AUC\": \"%.3f\" % auc_score})\n",
    "\n",
    "        fpr, tpr, roc_auc = exp.test()\n",
    "        normal_scores, anomaly_scores = exp.test_score_samples()\n",
    "        acc, prec, rec, f1 = exp.test_classification_metrics()\n",
    "        \n",
    "        roc.append((fpr, tpr))   \n",
    "        scores.append((normal_scores, anomaly_scores))\n",
    "        metrics[it] = [acc, prec, rec, f1, roc_auc]\n",
    "        \n",
    "    roc_df, scores_df, metrics_df = save_result(roc, scores, metrics, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_df.to_pickle(os.path.join(config['save_result_dir'], 'roc.pkl'))\n",
    "scores_df.to_pickle(os.path.join(config['save_result_dir'], 'sample_score.pkl'))\n",
    "metrics_df.to_csv(os.path.join(config['save_result_dir'], 'metrics.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal = df.iloc[:, 0]\n",
    "anomaly_scores = df.iloc[:, 1]\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(normal, bins=10, alpha=0.5, label='Normal')\n",
    "plt.hist(anomaly_scores, bins=10, alpha=0.5, label='Anomaly')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal = df.iloc[:, 4]\n",
    "anomaly_scores = df.iloc[:, 5]\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(normal, bins=10, alpha=0.5, label='Normal')\n",
    "plt.hist(anomaly_scores, bins=10, alpha=0.5, label='Anomaly')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = zip(*exp.test_dataset)\n",
    "X, y = np.stack(X).reshape(-1, 28*28), np.array(y)\n",
    "y = np.where(y == 1, -1, 1)\n",
    "\n",
    "y_predict = exp.model.predict(X)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y, y_predict)\n",
    "\n",
    "# accuracy, precision, recall, f1\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "accuracy_score(y, y_predict), precision_score(y, y_predict), recall_score(y, y_predict), f1_score(y, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(0.7933425797503467, 1.0, 0.6061674008810573, 0.754799780581459)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.score_samples(x_test)\n",
    "plt.hist(score, 25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Asumiendo que tienes las puntuaciones de predicción de tu modelo en y_scores\n",
    "fpr, tpr, thresholds = roc_curve(y_test, score)\n",
    "\n",
    "# Calcular la diferencia entre TPR y FPR para cada umbral\n",
    "differences = tpr - fpr\n",
    "\n",
    "# Encontrar el índice del umbral que maximiza la diferencia\n",
    "optimal_threshold_index = np.argmax(differences)\n",
    "\n",
    "# Obtener el umbral óptimo\n",
    "optimal_threshold = thresholds[optimal_threshold_index]\n",
    "\n",
    "print(\"Optimal threshold:\", optimal_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(score[np.argwhere(_y_test==1).squeeze()], bins=10, alpha=.3, label='normal')\n",
    "plt.hist(score[np.argwhere(_y_test==-1).squeeze()], bins=10, alpha=.3, label='anomaly')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Normalize,ToTensor, Compose\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "## Extract a 2 from the test dataset\n",
    "number = 2\n",
    "transform = Compose([ToTensor(), Normalize((0.5,), (0.5,))])\n",
    "test2_dataset = MNIST('data/', train = False, download = True, transform=transform)\n",
    "test2_idx = torch.where((test2_dataset.targets == number))[0]\n",
    "test2_dataset = Subset(test2_dataset, test2_idx)\n",
    "\n",
    "X, y = zip(*test2_dataset)\n",
    "X = torch.stack(X).reshape(-1, 28*28)\n",
    "y = torch.tensor(y).flatten()\n",
    "y_score = model.score_samples(X)\n",
    "# y_pred = np.zeros_like(y_score, dtype=np.int)\n",
    "# y_pred[y_score > 3] = 1\n",
    "# y_score = model(X).detach()[:,1]\n",
    "\n",
    "plt.hist(y_score, bins=10, alpha=.3, label='normal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_hat = model.predict(X)\n",
    "plt.hist(x_hat, bins=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = y_score > 1e-2\n",
    "np.unique(y_pred), np.bincount(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Tus datos\n",
    "data1 = score[_y_test==1]\n",
    "data2 = score[_y_test==-1]\n",
    "\n",
    "# Realizar la prueba t\n",
    "t_stat, p_value = stats.ttest_ind(data1, data2)\n",
    "\n",
    "print(f\"t-statistic: {t_stat}\")\n",
    "print(f\"p-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing OC-SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import PredefinedSplit\n",
    "\n",
    "seed = 42\n",
    "exp = ExperimentADeLEn(0, 0.1, seed=seed)\n",
    "x_train, y_train = zip(*[(_x, _y) for _x, _y in exp.train_dataset])\n",
    "x_train, y_train = torch.stack(x_train), torch.tensor(y_train)\n",
    "\n",
    "x_test, y_test = zip(*[(_x, _y) for _x, _y in exp.test_dataset])\n",
    "x_test, y_test = torch.stack(x_test), torch.tensor(y_test)\n",
    "# pick the 10% of the x_test, 5% of the anomalies and 5% of the normal samples\n",
    "percent = .05\n",
    "normal_idx = torch.argwhere(y_test == 0).flatten()\n",
    "n = int(len(normal_idx) * percent)\n",
    "idx = torch.randperm(len(normal_idx))\n",
    "normal_idx = normal_idx[idx[:n]]\n",
    "\n",
    "anomaly_idx = torch.argwhere(y_test == 1).flatten()\n",
    "n = int(len(anomaly_idx) * percent)\n",
    "idx = torch.randperm(len(anomaly_idx))\n",
    "anomaly_idx = anomaly_idx[idx[:n]]\n",
    "\n",
    "idx = torch.cat([normal_idx, anomaly_idx])\n",
    "x_test, y_test = x_test[idx], y_test[idx]\n",
    "\n",
    "\n",
    "# concatenate x_train and x_test\n",
    "X = torch.concat([x_train, x_test], dim=0).reshape(-1, 28*28)\n",
    "y = torch.concat([y_train, y_test], dim=0)\n",
    "y[y == 1] = -1\n",
    "y[y == 0] = 1\n",
    "\n",
    "\n",
    "test_fold = [-1]*len(x_train) + [0]*len(x_test)\n",
    "ps = PredefinedSplit(test_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "param_grid = {'kernel': ['rbf'], 'gamma': np.logspace(-3, 0, 10),\n",
    "                     'nu': [0.1, 0.3, 0.5, 0.9]}\n",
    "\n",
    "grid = GridSearchCV(OneClassSVM(), param_grid, scoring='f1', verbose=3, cv=ps, n_jobs=-1)\n",
    "grid.fit(X, y)\n",
    "best_params = grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def make_custom_scorer(X):\n",
    "    def score_oc_svm(estimator, X_test):\n",
    "        score = model.score_samples(X)\n",
    "        fpr, tpr, _ = roc_curve(y, score)\n",
    "        return auc(fpr, tpr)\n",
    "    return score_oc_svm\n",
    "\n",
    "custom_scorer = make_scorer(make_custom_scorer(x_test, y_test), greater_is_better=True)\n",
    "\n",
    "param_grid = {'kernel': ['rbf'], 'gamma': np.logspace(-3, 1, 8),\n",
    "                     'nu': [0.01, 0.05, 0.1, 0.5]}\n",
    "\n",
    "grid = GridSearchCV(OneClassSVM(), param_grid, refit=True, verbose=1, scoring=custom_scorer)\n",
    "grid.fit(_x_train)\n",
    "\n",
    "best_params = grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hypopt import GridSearch\n",
    "\n",
    "param_grid = {'kernel': ['rbf'], 'gamma': np.logspace(-3, 1, 8),\n",
    "                     'nu': [0.01, 0.05, 0.1, 0.5]}\n",
    "\n",
    "opt = GridSearch(model = OneClassSVM(), param_grid=param_grid, num_threads=2, parallelize=True)\n",
    "opt.fit(_x_train, y_train, x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(100, 2)\n",
    "y = np.random.randn(100, 2)\n",
    "model.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_custom_scorer(model, X, **kwargs):\n",
    "    def score_oc_svm(y_pred):\n",
    "        # Ignorar y_true y y_pred\n",
    "        # Usar model, X y y para calcular la puntuación\n",
    "        y_pred = model.predict(X)\n",
    "        score = 1\n",
    "        return score\n",
    "    return score_oc_svm\n",
    "\n",
    "custom_scorer = make_scorer(make_custom_scorer(model, x_test, y=y_test), greater_is_better=True)\n",
    "\n",
    "param_grid = {'kernel': ['rbf'], 'gamma': np.logspace(-3, 1, 8),\n",
    "                     'nu': [0.01, 0.05, 0.1, 0.5]}\n",
    "\n",
    "grid = GridSearchCV(OneClassSVM(), param_grid, refit=True, verbose=0, scoring='accuracy', n_jobs=1)\n",
    "grid.fit(_x_train)\n",
    "\n",
    "best_params = grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import OneClassSVM\n",
    "import numpy as np\n",
    "\n",
    "def custom_cross_val(X, y, model, param_grid, cv=5):\n",
    "    kf = KFold(n_splits=cv)\n",
    "    best_score = -1\n",
    "    best_params = None\n",
    "\n",
    "    for params in param_grid:\n",
    "        scores = []\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            model.set_params(**params)\n",
    "            model.fit(X_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            score = accuracy_score(y_test, y_pred)\n",
    "            scores.append(score)\n",
    "        avg_score = np.mean(scores)\n",
    "        if avg_score > best_score:\n",
    "            best_score = avg_score\n",
    "            best_params = params\n",
    "    return best_params, best_score\n",
    "\n",
    "model = OneClassSVM()\n",
    "param_grid = [{'kernel': ['rbf'], 'gamma': np.logspace(-3, 1, 8), 'nu': [0.01, 0.05, 0.1, 0.5]}]\n",
    "best_params, best_score = custom_cross_val(_x_train, y_train, model, param_grid, cv=5)\n",
    "\n",
    "print(\"Best parameters:\", best_params)\n",
    "print(\"Best score:\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.hist(score, bins=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation using training set for obtaining the best hyperparameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': np.logspace(-3, 1, 10),\n",
    "                     'nu': [0.1, 0.2, 0.3, 0.4, 0.5]}]\n",
    "\n",
    "scores = ['roc_auc_ovr']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(\n",
    "        OneClassSVM(), tuned_parameters, scoring=score\n",
    "    )\n",
    "    clf.fit(x_train.numpy())\n",
    "\n",
    "    # print(\"Best parameters set found on development set:\")\n",
    "    # print()\n",
    "    # print(clf.best_params_)\n",
    "    # print()\n",
    "    # print(\"Grid scores on development set:\")\n",
    "    # print()\n",
    "    # means = clf.cv_results_['mean_test_score']\n",
    "    # stds = clf.cv_results_['std_test_score']\n",
    "    # for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    #     print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "    #           % (mean, std * 2, params))\n",
    "    # print()\n",
    "\n",
    "    # print(\"Detailed classification report:\")\n",
    "    # print()\n",
    "    # print(\"The model is trained on the full development set.\")\n",
    "    # print(\"The scores are computed on the full evaluation set.\")\n",
    "    # print()\n",
    "    # y_true, y_pred = y_test, clf.predict(x_test.numpy())\n",
    "    # # print(classification_report(y_true, y_pred))\n",
    "    # # print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "# load the data\n",
    "digits = load_digits()\n",
    "\n",
    "# project the 64-dimensional data to a lower dimension\n",
    "pca = PCA(n_components=15, whiten=False)\n",
    "data = pca.fit_transform(digits.data)\n",
    "\n",
    "# use grid search cross-validation to optimize the bandwidth\n",
    "params = {\"bandwidth\": np.logspace(-1, 1, 20)}\n",
    "grid = GridSearchCV(KernelDensity(), params)\n",
    "grid.fit(data)\n",
    "\n",
    "print(\"best bandwidth: {0}\".format(grid.best_estimator_.bandwidth))\n",
    "\n",
    "# use the best estimator to compute the kernel density estimate\n",
    "kde = grid.best_estimator_\n",
    "\n",
    "# sample 44 new points from the data\n",
    "new_data = kde.sample(44, random_state=0)\n",
    "new_data = pca.inverse_transform(new_data)\n",
    "\n",
    "# turn data into a 4x11 grid\n",
    "new_data = new_data.reshape((4, 11, -1))\n",
    "real_data = digits.data[:44].reshape((4, 11, -1))\n",
    "\n",
    "# plot real digits and resampled digits\n",
    "fig, ax = plt.subplots(9, 11, subplot_kw=dict(xticks=[], yticks=[]))\n",
    "for j in range(11):\n",
    "    ax[4, j].set_visible(False)\n",
    "    for i in range(4):\n",
    "        im = ax[i, j].imshow(\n",
    "            real_data[i, j].reshape((8, 8)), cmap=plt.cm.binary, interpolation=\"nearest\"\n",
    "        )\n",
    "        im.set_clim(0, 16)\n",
    "        im = ax[i + 5, j].imshow(\n",
    "            new_data[i, j].reshape((8, 8)), cmap=plt.cm.binary, interpolation=\"nearest\"\n",
    "        )\n",
    "        im.set_clim(0, 16)\n",
    "\n",
    "ax[0, 5].set_title(\"Selection from the input data\")\n",
    "ax[5, 5].set_title('\"New\" digits drawn from the kernel density model')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = zip(*[(_x, _y) for _x, _y in exp.train_dataset])\n",
    "x_train, y_train = torch.stack(x_train), torch.tensor(y_train)\n",
    "\n",
    "x_train = x_train[y_train==0].view(-1, 28*28).numpy()\n",
    "y_train = y_train[y_train==0].numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomOneCLassSVM(OneClassSVM):\n",
    "    def __init__(self, X_test, y_test, kernel='rbf', gamma='scale',\n",
    "                 nu=0.5, **kwargs):\n",
    "        super().__init__(kernel=kernel, gamma=gamma, nu=nu, **kwargs)\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "    \n",
    "    def score(self, X, y=None):\n",
    "        from sklearn.metrics import roc_curve, auc\n",
    "        score = super().score_samples(self.X_test)\n",
    "        fpr, tpr, _ = roc_curve(self.y_test, score)\n",
    "        return auc(fpr, tpr)\n",
    "    \n",
    "\n",
    "param_grid = {'kernel': ['rbf'], 'gamma': np.logspace(-3, 1, 1),\n",
    "                     'nu': [0.01, 0.05, 0.1, 0.5]}\n",
    "\n",
    "_y_test = y_test.numpy()\n",
    "_y_test[_y_test == 1] = -1\n",
    "_y_test[_y_test == 0] = 1\n",
    "\n",
    "my_scorer = make_scorer(CustomOneCLassSVM.score)\n",
    "grid = GridSearchCV(CustomOneCLassSVM(x_test, y_test), param_grid, refit=True, verbose=2, n_jobs=1)\n",
    "grid.fit(x_train, y_train)\n",
    "\n",
    "best_params = grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = CustomOneCLassSVM(x_test, y_test, **best_params)\n",
    "model = CustomOneCLassSVM(x_test, y_test, kernel='rbf', gamma=.1, nu=1e-3)\n",
    "model.fit(x_train, y_train)\n",
    "model.score(None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MedMNIST Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from torchvision import transforms\n",
    "project_dir = os.path.join(os.getcwd(),'..')\n",
    "if project_dir not in sys.path:\n",
    "    sys.path.append(project_dir)\n",
    "\n",
    "from dataset.medmnist import AnomalyPneumoniaMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import AnomalyPneumoniaMNIST\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision import transforms\n",
    "import random\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[.5], std=[.5]),\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "# create a random seed\n",
    "seed = 128\n",
    "train_dataset = AnomalyPneumoniaMNIST('data/', download=True, transform=data_transform, n_normal_samples=-1, known_anomalies=0.2, pollution=0.0, seed=seed)\n",
    "print(train_dataset)\n",
    "\n",
    "train_dataset.montage(5, 5, seed)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = zip(*[(_x, _y) for _x, _y in train_dataset])\n",
    "x_train, y_train = torch.stack(x_train), torch.tensor(y_train)\n",
    "\n",
    "_x_train = x_train[y_train==0].view(-1, 28*28).numpy()\n",
    "_y_train = y_train[y_train==0].numpy()\n",
    "\n",
    "model = OneClassSVM(kernel='rbf', gamma=1e-2, nu=1e-3)\n",
    "model.fit(_x_train)\n",
    "\n",
    "# Predict\n",
    "x_test, y_test = zip(*[(_x, _y) for _x, _y in train_dataset])\n",
    "x_test, y_test = torch.stack(x_test), torch.tensor(y_test)\n",
    "\n",
    "_y_test = y_test.numpy()\n",
    "_y_test[_y_test == 1] = -1\n",
    "_y_test[_y_test == 0] = 1\n",
    "\n",
    "x_test = x_test.reshape(-1, 28*28)\n",
    "_y_pred = model.score_samples(x_test)\n",
    "\n",
    "# Evaluate\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "fpr, tpr, _ = roc_curve(_y_test, _y_pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot\n",
    "with plt.style.context((\"seaborn-colorblind\")):\n",
    "    fig = plt.figure(figsize=(6, 4))\n",
    "    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate', fontsize='x-large')\n",
    "    plt.ylabel('True Positive Rate', fontsize='x-large')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    plt.tick_params(axis='both', which='major', labelsize='large')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.score_samples(x_test)\n",
    "plt.hist(score, 25)\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Asumiendo que tienes las puntuaciones de predicción de tu modelo en y_scores\n",
    "fpr, tpr, thresholds = roc_curve(y_test, score)\n",
    "auc_score = auc(fpr, tpr)\n",
    "\n",
    "plt.plot(fpr, tpr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(score[torch.argwhere(y_test==1).squeeze()], bins=25, alpha=.5, label='normal')\n",
    "plt.hist(score[torch.argwhere(y_test==-1).squeeze()], bins=25, alpha=.5, label='anomaly')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular la diferencia entre TPR y FPR para cada umbral\n",
    "differences = tpr - fpr\n",
    "\n",
    "# Encontrar el índice del umbral que maximiza la diferencia\n",
    "optimal_threshold_index = np.argmax(differences)\n",
    "\n",
    "# Obtener el umbral óptimo\n",
    "optimal_threshold = thresholds[optimal_threshold_index]\n",
    "\n",
    "print(\"Optimal threshold:\", optimal_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Tus datos\n",
    "data1 = score[_y_test==1]\n",
    "data2 = score[_y_test==-1]\n",
    "\n",
    "# Realizar la prueba t\n",
    "t_stat, p_value = stats.ttest_ind(data1, data2)\n",
    "\n",
    "print(f\"t-statistic: {t_stat}\")\n",
    "print(f\"p-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "x_test, y_test = zip(*exp.test_dataset)\n",
    "x_test, y_test = torch.stack(x_test), torch.tensor(y_test)\n",
    "y_test = np.where(y_test.numpy()==0, 1, -1)\n",
    "x_test = x_test.reshape(-1, 28*28)\n",
    "\n",
    "score = exp.model.score_samples(x_test)\n",
    "plt.hist(score[np.argwhere(y_test==1).squeeze()], bins=10, alpha=.3, label='normal', density=False)\n",
    "plt.hist(score[np.argwhere(y_test==-1).squeeze()], bins=10, alpha=.3, label='anomaly', density=False)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = exp.model.predict(x_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Normalize,ToTensor, Compose\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "## Extract a 2 from the test dataset\n",
    "number = 0\n",
    "transform = Compose([ToTensor(), Normalize((0.5,), (0.5,))])\n",
    "test2_dataset = MNIST('data/', train = False, download = True, transform=transform)\n",
    "test2_idx = torch.where((test2_dataset.targets == number))[0]\n",
    "test2_dataset = Subset(test2_dataset, test2_idx)\n",
    "\n",
    "X, y = zip(*test2_dataset)\n",
    "X = torch.stack(X).reshape(-1, 28*28)\n",
    "y = torch.tensor(y).flatten()\n",
    "y_score = exp.model.score_samples(X)\n",
    "# y_pred = np.zeros_like(y_score, dtype=np.int)\n",
    "# y_pred[y_score > 3] = 1\n",
    "# y_score = model(X).detach()[:,1]\n",
    "\n",
    "plt.hist(y_score, bins=10, alpha=.3, label='normal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_hat = exp.model.predict(X)\n",
    "plt.hist(x_hat, bins=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
