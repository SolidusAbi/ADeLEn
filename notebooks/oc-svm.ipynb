{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "project_dir = os.path.join(os.getcwd(),'..')\n",
    "if project_dir not in sys.path:\n",
    "    sys.path.append(project_dir)\n",
    "\n",
    "from experiments.MNIST import ExperimentSVM\n",
    "from experiments.utils import generate_roc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "def generate_roc_df(roc_list:list) -> pd.DataFrame:\n",
    "    ''' \n",
    "        Create a DataFrame from a list of roc curves\n",
    "        Args:\n",
    "        -----\n",
    "            roc_list: list\n",
    "                List of N roc curves where N is the number of iterations. It is a \n",
    "                list of tuples of the form (fpr, tpr), where fpr is the false positive\n",
    "                rate and tpr is the true positive rate.\n",
    "        Returns:\n",
    "        --------\n",
    "            roc_df: pd.DataFrame\n",
    "                DataFrame with multiindex\n",
    "    '''\n",
    "    index_names = [\n",
    "        list(map(lambda x: 'It {}'.format(x), np.repeat(np.arange(len(roc_list)), 2) + 1 )),\n",
    "        ['FPR', 'TPR']*len(roc_list)\n",
    "    ]\n",
    "        \n",
    "    tuples = list(zip(*index_names))\n",
    "    index = pd.MultiIndex.from_tuples(tuples)\n",
    "    roc_df = pd.DataFrame(chain.from_iterable(roc_list), index=index)\n",
    "    return roc_df\n",
    "\n",
    "def generate_multi_df(data:list, index_names:list) -> pd.DataFrame:\n",
    "    index_names = [\n",
    "        list(map(lambda x: 'It {}'.format(x), np.repeat(np.arange(len(data)), len(index_names)) + 1 )),\n",
    "        index_names*len(data)\n",
    "    ]\n",
    "\n",
    "    tuples = list(zip(*index_names))\n",
    "    index = pd.MultiIndex.from_tuples(tuples)\n",
    "    return pd.DataFrame(chain.from_iterable(data), index=index)\n",
    "\n",
    "def save_result(roc:list, scores:list, metrics:np.ndarray, config:dict) -> tuple:\n",
    "    '''\n",
    "        Save the results of the experiment\n",
    "        Args:\n",
    "        -----\n",
    "            roc: list\n",
    "                List of roc curves\n",
    "            auc: list\n",
    "                List of AUC scores\n",
    "            config: dict\n",
    "                Configuration of the experiment\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "            roc_df: pd.DataFrame\n",
    "                ROC Cuve dataFrame with multiindex, considering the iterations.\n",
    "            auc_df: pd.DataFrame\n",
    "                DataFrame with the AUC scores\n",
    "    '''\n",
    "\n",
    "    roc_df = generate_multi_df(roc, ['FPR', 'TPR']).T\n",
    "    scores_df = generate_multi_df(scores, ['Normal', 'Anomaly']).T\n",
    "    metrics_df = pd.DataFrame(metrics, columns=['Accuracy', 'Precision', 'Recall', 'F1', 'AUC'])\n",
    "    \n",
    "    roc_df.to_pickle(os.path.join(config['save_result_dir'], 'roc.pkl'))\n",
    "    scores_df.to_pickle(os.path.join(config['save_result_dir'], 'sample_score.pkl'))\n",
    "    metrics_df.to_csv(os.path.join(config['save_result_dir'], 'metrics.csv'))\n",
    "\n",
    "    return roc_df, scores_df, metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 3\n",
    "seed = 2*np.arange(n_iter, dtype=int) + 42\n",
    "iterator = tqdm(\n",
    "            range(n_iter),\n",
    "            leave=True,\n",
    "            unit=\"It.\",\n",
    "            postfix={\"AUC\": \"%.3f\" % -1},\n",
    "        )\n",
    "\n",
    "roc, scores = [], []\n",
    "metrics = np.empty((n_iter, 5)) # acc, prec, rec, f1, auc\n",
    "\n",
    "for it in iterator:\n",
    "    exp = ExperimentSVM(known_anomalies=.1, pollution=0.1, seed=int(seed[it]))\n",
    "    if it == 0:\n",
    "        config = exp.config()\n",
    "        exp.save_config()\n",
    "    \n",
    "    \n",
    "    auc_score = exp.run(verbose=0)\n",
    "    iterator.set_postfix({\"AUC\": \"%.3f\" % auc_score})\n",
    "\n",
    "    fpr, tpr, roc_auc = exp.test()\n",
    "    normal_scores, anomaly_scores = exp.test_score_samples()\n",
    "    acc, prec, rec, f1 = exp.test_classification_metrics()\n",
    "    \n",
    "    roc.append((fpr, tpr))   \n",
    "    scores.append((normal_scores, anomaly_scores))\n",
    "    metrics[it] = [acc, prec, rec, f1, roc_auc]\n",
    "    \n",
    "roc_df, scores_df, metrics_df = save_result(roc, scores, metrics, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_df.to_pickle(os.path.join(config['save_result_dir'], 'roc.pkl'))\n",
    "scores_df.to_pickle(os.path.join(config['save_result_dir'], 'sample_score.pkl'))\n",
    "metrics_df.to_csv(os.path.join(config['save_result_dir'], 'metrics.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_score = scores_df['It 3', 'Normal'].values\n",
    "anomaly_scores = scores_df['It 3', 'Anomaly'].values\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(normal_score, bins=10, alpha=0.5, label='Normal')\n",
    "plt.hist(anomaly_scores, bins=10, alpha=0.5, label='Anomaly')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Normalize,ToTensor, Compose\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "## Extract a 2 from the test dataset\n",
    "number = 2\n",
    "transform = Compose([ToTensor(), Normalize((0.5,), (0.5,))])\n",
    "test2_dataset = MNIST('data/', train = False, download = True, transform=transform)\n",
    "test2_idx = torch.where((test2_dataset.targets == number))[0]\n",
    "test2_dataset = Subset(test2_dataset, test2_idx)\n",
    "\n",
    "X, y = zip(*test2_dataset)\n",
    "X = torch.stack(X).reshape(-1, 28*28)\n",
    "y = torch.tensor(y).flatten()\n",
    "y_score = exp.model.score_samples(X)\n",
    "# y_pred = np.zeros_like(y_score, dtype=np.int)\n",
    "# y_pred[y_score > 3] = 1\n",
    "# y_score = model(X).detach()[:,1]\n",
    "\n",
    "plt.hist(y_score, bins=10, alpha=.3, label='normal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_hat = exp.model.predict(X)\n",
    "plt.hist(x_hat, bins=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = y_score > 1e-2\n",
    "np.unique(y_pred), np.bincount(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Tus datos\n",
    "data1 = score[_y_test==1]\n",
    "data2 = score[_y_test==-1]\n",
    "\n",
    "# Realizar la prueba t\n",
    "t_stat, p_value = stats.ttest_ind(data1, data2)\n",
    "\n",
    "print(f\"t-statistic: {t_stat}\")\n",
    "print(f\"p-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MedMNIST Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from torchvision import transforms\n",
    "project_dir = os.path.join(os.getcwd(),'..')\n",
    "if project_dir not in sys.path:\n",
    "    sys.path.append(project_dir)\n",
    "\n",
    "from dataset.medmnist import AnomalyPneumoniaMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import AnomalyPneumoniaMNIST\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision import transforms\n",
    "import random\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[.5], std=[.5]),\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "# create a random seed\n",
    "seed = 128\n",
    "train_dataset = AnomalyPneumoniaMNIST('data/', download=True, transform=data_transform, n_normal_samples=-1, known_anomalies=0.2, pollution=0.0, seed=seed)\n",
    "print(train_dataset)\n",
    "\n",
    "train_dataset.montage(5, 5, seed)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = zip(*[(_x, _y) for _x, _y in train_dataset])\n",
    "x_train, y_train = torch.stack(x_train), torch.tensor(y_train)\n",
    "\n",
    "_x_train = x_train[y_train==0].view(-1, 28*28).numpy()\n",
    "_y_train = y_train[y_train==0].numpy()\n",
    "\n",
    "model = OneClassSVM(kernel='rbf', gamma=1e-2, nu=1e-3)\n",
    "model.fit(_x_train)\n",
    "\n",
    "# Predict\n",
    "x_test, y_test = zip(*[(_x, _y) for _x, _y in train_dataset])\n",
    "x_test, y_test = torch.stack(x_test), torch.tensor(y_test)\n",
    "\n",
    "_y_test = y_test.numpy()\n",
    "_y_test[_y_test == 1] = -1\n",
    "_y_test[_y_test == 0] = 1\n",
    "\n",
    "x_test = x_test.reshape(-1, 28*28)\n",
    "_y_pred = model.score_samples(x_test)\n",
    "\n",
    "# Evaluate\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "fpr, tpr, _ = roc_curve(_y_test, _y_pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot\n",
    "with plt.style.context((\"seaborn-colorblind\")):\n",
    "    fig = plt.figure(figsize=(6, 4))\n",
    "    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate', fontsize='x-large')\n",
    "    plt.ylabel('True Positive Rate', fontsize='x-large')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    plt.tick_params(axis='both', which='major', labelsize='large')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.score_samples(x_test)\n",
    "plt.hist(score, 25)\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Asumiendo que tienes las puntuaciones de predicción de tu modelo en y_scores\n",
    "fpr, tpr, thresholds = roc_curve(y_test, score)\n",
    "auc_score = auc(fpr, tpr)\n",
    "\n",
    "plt.plot(fpr, tpr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(score[torch.argwhere(y_test==1).squeeze()], bins=25, alpha=.5, label='normal')\n",
    "plt.hist(score[torch.argwhere(y_test==-1).squeeze()], bins=25, alpha=.5, label='anomaly')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular la diferencia entre TPR y FPR para cada umbral\n",
    "differences = tpr - fpr\n",
    "\n",
    "# Encontrar el índice del umbral que maximiza la diferencia\n",
    "optimal_threshold_index = np.argmax(differences)\n",
    "\n",
    "# Obtener el umbral óptimo\n",
    "optimal_threshold = thresholds[optimal_threshold_index]\n",
    "\n",
    "print(\"Optimal threshold:\", optimal_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Tus datos\n",
    "data1 = score[_y_test==1]\n",
    "data2 = score[_y_test==-1]\n",
    "\n",
    "# Realizar la prueba t\n",
    "t_stat, p_value = stats.ttest_ind(data1, data2)\n",
    "\n",
    "print(f\"t-statistic: {t_stat}\")\n",
    "print(f\"p-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "x_test, y_test = zip(*exp.test_dataset)\n",
    "x_test, y_test = torch.stack(x_test), torch.tensor(y_test)\n",
    "y_test = np.where(y_test.numpy()==0, 1, -1)\n",
    "x_test = x_test.reshape(-1, 28*28)\n",
    "\n",
    "score = exp.model.score_samples(x_test)\n",
    "plt.hist(score[np.argwhere(y_test==1).squeeze()], bins=10, alpha=.3, label='normal', density=False)\n",
    "plt.hist(score[np.argwhere(y_test==-1).squeeze()], bins=10, alpha=.3, label='anomaly', density=False)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = exp.model.predict(x_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Normalize,ToTensor, Compose\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "## Extract a 2 from the test dataset\n",
    "number = 0\n",
    "transform = Compose([ToTensor(), Normalize((0.5,), (0.5,))])\n",
    "test2_dataset = MNIST('data/', train = False, download = True, transform=transform)\n",
    "test2_idx = torch.where((test2_dataset.targets == number))[0]\n",
    "test2_dataset = Subset(test2_dataset, test2_idx)\n",
    "\n",
    "X, y = zip(*test2_dataset)\n",
    "X = torch.stack(X).reshape(-1, 28*28)\n",
    "y = torch.tensor(y).flatten()\n",
    "y_score = exp.model.score_samples(X)\n",
    "# y_pred = np.zeros_like(y_score, dtype=np.int)\n",
    "# y_pred[y_score > 3] = 1\n",
    "# y_score = model(X).detach()[:,1]\n",
    "\n",
    "plt.hist(y_score, bins=10, alpha=.3, label='normal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_hat = exp.model.predict(X)\n",
    "plt.hist(x_hat, bins=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
