{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "project_dir = os.path.join(os.getcwd(),'..')\n",
    "if project_dir not in sys.path:\n",
    "    sys.path.append(project_dir)\n",
    "from experiments.MNIST import ExperimentMNIST\n",
    "\n",
    "\n",
    "\n",
    "from experiments.MNIST import ExperimentMNIST\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "\n",
    "import random\n",
    "# seed = random.randint(0, 1000)\n",
    "seed = 10\n",
    "exp = ExperimentMNIST(0.1, 0.2, seed=seed)\n",
    "\n",
    "# Create a model\n",
    "model = OneClassSVM(kernel='rbf', nu=.6, gamma=1e-3)\n",
    "x_train, y_train = zip(*[(_x, _y) for _x, _y in exp.train_dataset])\n",
    "x_train, y_train = torch.stack(x_train), torch.tensor(y_train)\n",
    "\n",
    "x_train = x_train.reshape(-1, 28*28)\n",
    "_x_train = x_train.numpy()\n",
    "\n",
    "_x_train = _x_train[y_train == 0]\n",
    "# randperm 1000 samples\n",
    "_x_train = _x_train[np.random.permutation(_x_train.shape[0])[:2000]]\n",
    "some_anomalies = x_train[y_train == 1].numpy()\n",
    "anomalies = np.random.permutation(some_anomalies.shape[0])[:100]\n",
    "\n",
    "_x_train = np.concatenate([_x_train, some_anomalies[anomalies]])\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "model.fit(_x_train)\n",
    "\n",
    "# Predict\n",
    "x_test, y_test = zip(*[(_x, _y) for _x, _y in exp.test_dataset])\n",
    "x_test, y_test = torch.stack(x_test), torch.tensor(y_test)\n",
    "\n",
    "_y_test = y_test.numpy()\n",
    "_y_test[_y_test == 1] = -1\n",
    "_y_test[_y_test == 0] = 1\n",
    "\n",
    "x_test = x_test.reshape(-1, 28*28)\n",
    "y_pred = model.score_samples(x_test)\n",
    "\n",
    "# Evaluate\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "fpr, tpr, _ = roc_curve(_y_test, y_pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot\n",
    "with plt.style.context((\"seaborn-colorblind\")):\n",
    "    fig = plt.figure(figsize=(6, 4))\n",
    "    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate', fontsize='x-large')\n",
    "    plt.ylabel('True Positive Rate', fontsize='x-large')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    plt.tick_params(axis='both', which='major', labelsize='large')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(_y_test, y_pred)\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.score_samples(x_test)\n",
    "plt.hist(score, 25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Asumiendo que tienes las puntuaciones de predicción de tu modelo en y_scores\n",
    "fpr, tpr, thresholds = roc_curve(y_test, score)\n",
    "\n",
    "# Calcular la diferencia entre TPR y FPR para cada umbral\n",
    "differences = tpr - fpr\n",
    "\n",
    "# Encontrar el índice del umbral que maximiza la diferencia\n",
    "optimal_threshold_index = np.argmax(differences)\n",
    "\n",
    "# Obtener el umbral óptimo\n",
    "optimal_threshold = thresholds[optimal_threshold_index]\n",
    "\n",
    "print(\"Optimal threshold:\", optimal_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Asumiendo que tienes las puntuaciones de predicción de tu modelo en y_scores\n",
    "fpr, tpr, thresholds = roc_curve(y_test, score)\n",
    "\n",
    "# Calcular la diferencia entre TPR y FPR para cada umbral\n",
    "differences = tpr - fpr\n",
    "\n",
    "# Encontrar el índice del umbral que maximiza la diferencia\n",
    "optimal_threshold_index = np.argmax(differences)\n",
    "\n",
    "# Obtener el umbral óptimo\n",
    "optimal_threshold = thresholds[optimal_threshold_index]\n",
    "\n",
    "print(\"Optimal threshold:\", optimal_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(score[np.argwhere(_y_test==1).squeeze()], bins=10, alpha=.3, label='normal')\n",
    "plt.hist(score[np.argwhere(_y_test==-1).squeeze()], bins=10, alpha=.3, label='anomaly')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Normalize,ToTensor, Compose\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "## Extract a 2 from the test dataset\n",
    "number = 0\n",
    "transform = Compose([ToTensor(), Normalize((0.5,), (0.5,))])\n",
    "test2_dataset = MNIST('data/', train = False, download = True, transform=transform)\n",
    "test2_idx = torch.where((test2_dataset.targets == number))[0]\n",
    "test2_dataset = Subset(test2_dataset, test2_idx)\n",
    "\n",
    "X, y = zip(*test2_dataset)\n",
    "X = torch.stack(X).reshape(-1, 28*28)\n",
    "y = torch.tensor(y).flatten()\n",
    "y_score = model.score_samples(X)\n",
    "# y_pred = np.zeros_like(y_score, dtype=np.int)\n",
    "# y_pred[y_score > 3] = 1\n",
    "# y_score = model(X).detach()[:,1]\n",
    "\n",
    "plt.hist(y_score, bins=10, alpha=.3, label='normal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_hat = model.predict(X)\n",
    "plt.hist(x_hat, bins=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = y_score > 1e-2\n",
    "np.unique(y_pred), np.bincount(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Tus datos\n",
    "data1 = score[_y_test==1]\n",
    "data2 = score[_y_test==-1]\n",
    "\n",
    "# Realizar la prueba t\n",
    "t_stat, p_value = stats.ttest_ind(data1, data2)\n",
    "\n",
    "print(f\"t-statistic: {t_stat}\")\n",
    "print(f\"p-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score[np.argwhere(_y_test==-1).squeeze()].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = zip(*[(x,y)for x, y in exp.test_dataset])\n",
    "x = torch.stack(x)\n",
    "y = torch.tensor(y)\n",
    "\n",
    "percent = .1\n",
    "\n",
    "normal_idx = torch.argwhere(y == 0).flatten()\n",
    "n = int(len(normal_idx) * percent)\n",
    "idx = torch.randperm(len(normal_idx))\n",
    "normal_idx = normal_idx[idx[:n]]\n",
    "\n",
    "anomaly_idx = torch.argwhere(y == 1).flatten()\n",
    "n = int(len(anomaly_idx) * percent)\n",
    "idx = torch.randperm(len(anomaly_idx))\n",
    "anomaly_idx = anomaly_idx[idx[:n]]\n",
    "\n",
    "idx = torch.cat([normal_idx, anomaly_idx])\n",
    "x_test, y_test = x[idx], y[idx]\n",
    "x_test = x_test.reshape(len(y_test), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.shape, y.shape)\n",
    "y[normal_idx].shape, y[anomaly_idx].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = torch.cat([normal_idx, anomaly_idx])\n",
    "# x_train = x[idx]\n",
    "# y_train = y[idx]\n",
    "\n",
    "# model = OneClassSVM(kernel='rbf', nu=0.05, gamma='auto')\n",
    "# x_train = x_train.reshape(-1, 28*28)\n",
    "# _x_train = x_train.numpy()\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# _x_train = scaler.fit_transform(_x_train)\n",
    "\n",
    "# model.fit(_x_train)\n",
    "# x_test = x.reshape(-1, 28*28)\n",
    "# _x_test = scaler.transform(x_test)\n",
    "\n",
    "# y_pred = model.predict(_x_test)\n",
    "# fpr, tpr, _ = roc_curve(y, y_pred)\n",
    "# roc_auc = auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing OC-SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 10\n",
    "exp = ExperimentMNIST(0.1, 0.2, seed=seed)\n",
    "\n",
    "x_train, y_train = zip(*[(_x, _y) for _x, _y in exp.train_dataset])\n",
    "x_train, y_train = torch.stack(x_train), torch.tensor(y_train)\n",
    "\n",
    "x_train = x_train.reshape(-1, 28*28)\n",
    "_x_train = x_train.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "param_grid = {'kernel': ['rbf'], 'gamma': np.logspace(-3, 1, 8),\n",
    "                     'nu': [0.1, 0.3, 0.5, 0.9]}\n",
    "\n",
    "grid = GridSearchCV(OneClassSVM(), param_grid, scoring='accuracy', verbose=2, cv=5, n_jobs=-1)\n",
    "grid.fit(_x_train, y_train)\n",
    "best_params = grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def make_custom_scorer(X):\n",
    "    def score_oc_svm(estimator, X_test):\n",
    "        score = model.score_samples(X)\n",
    "        fpr, tpr, _ = roc_curve(y, score)\n",
    "        return auc(fpr, tpr)\n",
    "    return score_oc_svm\n",
    "\n",
    "custom_scorer = make_scorer(make_custom_scorer(x_test, y_test), greater_is_better=True)\n",
    "\n",
    "param_grid = {'kernel': ['rbf'], 'gamma': np.logspace(-3, 1, 8),\n",
    "                     'nu': [0.01, 0.05, 0.1, 0.5]}\n",
    "\n",
    "grid = GridSearchCV(OneClassSVM(), param_grid, refit=True, verbose=1, scoring=custom_scorer)\n",
    "grid.fit(_x_train)\n",
    "\n",
    "best_params = grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hypopt import GridSearch\n",
    "\n",
    "param_grid = {'kernel': ['rbf'], 'gamma': np.logspace(-3, 1, 8),\n",
    "                     'nu': [0.01, 0.05, 0.1, 0.5]}\n",
    "\n",
    "opt = GridSearch(model = OneClassSVM(), param_grid=param_grid, num_threads=2, parallelize=True)\n",
    "opt.fit(_x_train, y_train, x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(100, 2)\n",
    "y = np.random.randn(100, 2)\n",
    "model.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_custom_scorer(model, X, **kwargs):\n",
    "    def score_oc_svm(y_pred):\n",
    "        # Ignorar y_true y y_pred\n",
    "        # Usar model, X y y para calcular la puntuación\n",
    "        y_pred = model.predict(X)\n",
    "        score = 1\n",
    "        return score\n",
    "    return score_oc_svm\n",
    "\n",
    "custom_scorer = make_scorer(make_custom_scorer(model, x_test, y=y_test), greater_is_better=True)\n",
    "\n",
    "param_grid = {'kernel': ['rbf'], 'gamma': np.logspace(-3, 1, 8),\n",
    "                     'nu': [0.01, 0.05, 0.1, 0.5]}\n",
    "\n",
    "grid = GridSearchCV(OneClassSVM(), param_grid, refit=True, verbose=0, scoring='accuracy', n_jobs=1)\n",
    "grid.fit(_x_train)\n",
    "\n",
    "best_params = grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import OneClassSVM\n",
    "import numpy as np\n",
    "\n",
    "def custom_cross_val(X, y, model, param_grid, cv=5):\n",
    "    kf = KFold(n_splits=cv)\n",
    "    best_score = -1\n",
    "    best_params = None\n",
    "\n",
    "    for params in param_grid:\n",
    "        scores = []\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            model.set_params(**params)\n",
    "            model.fit(X_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            score = accuracy_score(y_test, y_pred)\n",
    "            scores.append(score)\n",
    "        avg_score = np.mean(scores)\n",
    "        if avg_score > best_score:\n",
    "            best_score = avg_score\n",
    "            best_params = params\n",
    "    return best_params, best_score\n",
    "\n",
    "model = OneClassSVM()\n",
    "param_grid = [{'kernel': ['rbf'], 'gamma': np.logspace(-3, 1, 8), 'nu': [0.01, 0.05, 0.1, 0.5]}]\n",
    "best_params, best_score = custom_cross_val(_x_train, y_train, model, param_grid, cv=5)\n",
    "\n",
    "print(\"Best parameters:\", best_params)\n",
    "print(\"Best score:\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.hist(score, bins=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation using training set for obtaining the best hyperparameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': np.logspace(-3, 1, 10),\n",
    "                     'nu': [0.1, 0.2, 0.3, 0.4, 0.5]}]\n",
    "\n",
    "scores = ['roc_auc_ovr']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(\n",
    "        OneClassSVM(), tuned_parameters, scoring=score\n",
    "    )\n",
    "    clf.fit(x_train.numpy())\n",
    "\n",
    "    # print(\"Best parameters set found on development set:\")\n",
    "    # print()\n",
    "    # print(clf.best_params_)\n",
    "    # print()\n",
    "    # print(\"Grid scores on development set:\")\n",
    "    # print()\n",
    "    # means = clf.cv_results_['mean_test_score']\n",
    "    # stds = clf.cv_results_['std_test_score']\n",
    "    # for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    #     print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "    #           % (mean, std * 2, params))\n",
    "    # print()\n",
    "\n",
    "    # print(\"Detailed classification report:\")\n",
    "    # print()\n",
    "    # print(\"The model is trained on the full development set.\")\n",
    "    # print(\"The scores are computed on the full evaluation set.\")\n",
    "    # print()\n",
    "    # y_true, y_pred = y_test, clf.predict(x_test.numpy())\n",
    "    # # print(classification_report(y_true, y_pred))\n",
    "    # # print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "# load the data\n",
    "digits = load_digits()\n",
    "\n",
    "# project the 64-dimensional data to a lower dimension\n",
    "pca = PCA(n_components=15, whiten=False)\n",
    "data = pca.fit_transform(digits.data)\n",
    "\n",
    "# use grid search cross-validation to optimize the bandwidth\n",
    "params = {\"bandwidth\": np.logspace(-1, 1, 20)}\n",
    "grid = GridSearchCV(KernelDensity(), params)\n",
    "grid.fit(data)\n",
    "\n",
    "print(\"best bandwidth: {0}\".format(grid.best_estimator_.bandwidth))\n",
    "\n",
    "# use the best estimator to compute the kernel density estimate\n",
    "kde = grid.best_estimator_\n",
    "\n",
    "# sample 44 new points from the data\n",
    "new_data = kde.sample(44, random_state=0)\n",
    "new_data = pca.inverse_transform(new_data)\n",
    "\n",
    "# turn data into a 4x11 grid\n",
    "new_data = new_data.reshape((4, 11, -1))\n",
    "real_data = digits.data[:44].reshape((4, 11, -1))\n",
    "\n",
    "# plot real digits and resampled digits\n",
    "fig, ax = plt.subplots(9, 11, subplot_kw=dict(xticks=[], yticks=[]))\n",
    "for j in range(11):\n",
    "    ax[4, j].set_visible(False)\n",
    "    for i in range(4):\n",
    "        im = ax[i, j].imshow(\n",
    "            real_data[i, j].reshape((8, 8)), cmap=plt.cm.binary, interpolation=\"nearest\"\n",
    "        )\n",
    "        im.set_clim(0, 16)\n",
    "        im = ax[i + 5, j].imshow(\n",
    "            new_data[i, j].reshape((8, 8)), cmap=plt.cm.binary, interpolation=\"nearest\"\n",
    "        )\n",
    "        im.set_clim(0, 16)\n",
    "\n",
    "ax[0, 5].set_title(\"Selection from the input data\")\n",
    "ax[5, 5].set_title('\"New\" digits drawn from the kernel density model')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = zip(*[(_x, _y) for _x, _y in exp.train_dataset])\n",
    "x_train, y_train = torch.stack(x_train), torch.tensor(y_train)\n",
    "\n",
    "x_train = x_train[y_train==0].view(-1, 28*28).numpy()\n",
    "y_train = y_train[y_train==0].numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomOneCLassSVM(OneClassSVM):\n",
    "    def __init__(self, X_test, y_test, kernel='rbf', gamma='scale',\n",
    "                 nu=0.5, **kwargs):\n",
    "        super().__init__(kernel=kernel, gamma=gamma, nu=nu, **kwargs)\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "    \n",
    "    def score(self, X, y=None):\n",
    "        from sklearn.metrics import roc_curve, auc\n",
    "        score = super().score_samples(self.X_test)\n",
    "        fpr, tpr, _ = roc_curve(self.y_test, score)\n",
    "        return auc(fpr, tpr)\n",
    "    \n",
    "\n",
    "param_grid = {'kernel': ['rbf'], 'gamma': np.logspace(-3, 1, 1),\n",
    "                     'nu': [0.01, 0.05, 0.1, 0.5]}\n",
    "\n",
    "_y_test = y_test.numpy()\n",
    "_y_test[_y_test == 1] = -1\n",
    "_y_test[_y_test == 0] = 1\n",
    "\n",
    "my_scorer = make_scorer(CustomOneCLassSVM.score)\n",
    "grid = GridSearchCV(CustomOneCLassSVM(x_test, y_test), param_grid, refit=True, verbose=2, n_jobs=1)\n",
    "grid.fit(x_train, y_train)\n",
    "\n",
    "best_params = grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = CustomOneCLassSVM(x_test, y_test, **best_params)\n",
    "model = CustomOneCLassSVM(x_test, y_test, kernel='rbf', gamma=.1, nu=1e-3)\n",
    "model.fit(x_train, y_train)\n",
    "model.score(None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MedMNIST Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from torchvision import transforms\n",
    "project_dir = os.path.join(os.getcwd(),'..')\n",
    "if project_dir not in sys.path:\n",
    "    sys.path.append(project_dir)\n",
    "\n",
    "from dataset.medmnist import AnomalyPneumoniaMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import AnomalyPneumoniaMNIST\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision import transforms\n",
    "import random\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[.5], std=[.5]),\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "# create a random seed\n",
    "seed = 128\n",
    "train_dataset = AnomalyPneumoniaMNIST('data/', download=True, transform=data_transform, n_normal_samples=-1, known_anomalies=0.2, pollution=0.0, seed=seed)\n",
    "print(train_dataset)\n",
    "\n",
    "train_dataset.montage(5, 5, seed)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = zip(*[(_x, _y) for _x, _y in train_dataset])\n",
    "x_train, y_train = torch.stack(x_train), torch.tensor(y_train)\n",
    "\n",
    "_x_train = x_train[y_train==0].view(-1, 28*28).numpy()\n",
    "_y_train = y_train[y_train==0].numpy()\n",
    "\n",
    "model = OneClassSVM(kernel='rbf', gamma=1e-2, nu=1e-3)\n",
    "model.fit(_x_train)\n",
    "\n",
    "# Predict\n",
    "x_test, y_test = zip(*[(_x, _y) for _x, _y in train_dataset])\n",
    "x_test, y_test = torch.stack(x_test), torch.tensor(y_test)\n",
    "\n",
    "_y_test = y_test.numpy()\n",
    "_y_test[_y_test == 1] = -1\n",
    "_y_test[_y_test == 0] = 1\n",
    "\n",
    "x_test = x_test.reshape(-1, 28*28)\n",
    "_y_pred = model.score_samples(x_test)\n",
    "\n",
    "# Evaluate\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "fpr, tpr, _ = roc_curve(_y_test, _y_pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot\n",
    "with plt.style.context((\"seaborn-colorblind\")):\n",
    "    fig = plt.figure(figsize=(6, 4))\n",
    "    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate', fontsize='x-large')\n",
    "    plt.ylabel('True Positive Rate', fontsize='x-large')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    plt.tick_params(axis='both', which='major', labelsize='large')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.score_samples(x_test)\n",
    "plt.hist(score, 25)\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Asumiendo que tienes las puntuaciones de predicción de tu modelo en y_scores\n",
    "fpr, tpr, thresholds = roc_curve(y_test, score)\n",
    "auc_score = auc(fpr, tpr)\n",
    "\n",
    "plt.plot(fpr, tpr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(score[torch.argwhere(y_test==1).squeeze()], bins=25, alpha=.5, label='normal')\n",
    "plt.hist(score[torch.argwhere(y_test==-1).squeeze()], bins=25, alpha=.5, label='anomaly')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular la diferencia entre TPR y FPR para cada umbral\n",
    "differences = tpr - fpr\n",
    "\n",
    "# Encontrar el índice del umbral que maximiza la diferencia\n",
    "optimal_threshold_index = np.argmax(differences)\n",
    "\n",
    "# Obtener el umbral óptimo\n",
    "optimal_threshold = thresholds[optimal_threshold_index]\n",
    "\n",
    "print(\"Optimal threshold:\", optimal_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Tus datos\n",
    "data1 = score[_y_test==1]\n",
    "data2 = score[_y_test==-1]\n",
    "\n",
    "# Realizar la prueba t\n",
    "t_stat, p_value = stats.ttest_ind(data1, data2)\n",
    "\n",
    "print(f\"t-statistic: {t_stat}\")\n",
    "print(f\"p-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
