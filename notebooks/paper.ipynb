{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "project_dir = os.path.join(os.getcwd(),'..')\n",
    "if project_dir not in sys.path:\n",
    "    sys.path.append(project_dir)\n",
    "import config\n",
    "\n",
    "from experiments.MNIST import ExperimentADeLEn, ExperimentSupervised, ExperimentSVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pollution experiments using MNIST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pollution_exp = [0, .05, .1, .2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "def generate_multi_df(data:list, parent_index:list, child_index:list) -> pd.DataFrame:\n",
    "    index_names = [\n",
    "        [x for x in parent_index for _ in child_index],\n",
    "        child_index*len(parent_index)\n",
    "    ]\n",
    "\n",
    "    tuples = list(zip(*index_names))\n",
    "    index = pd.MultiIndex.from_tuples(tuples)\n",
    "    return pd.DataFrame(chain.from_iterable(data), index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = ExperimentADeLEn(.1, .2, 2)\n",
    "result_dir = exp.config()['save_result_dir']\n",
    "df = pd.read_csv(os.path.join(result_dir, 'metrics.csv'), index_col=0)\n",
    "acc = df['Accuracy'].mean(), df['Accuracy'].std()\n",
    "prec = df['Precision'].mean(), df['Precision'].std()\n",
    "rec = df['Recall'].mean(), df['Recall'].std()\n",
    "f1 = df['F1'].mean(), df['F1'].std()\n",
    "auc = df['AUC'].mean(), df['AUC'].std()\n",
    "\n",
    "\n",
    "data = [acc, prec, rec, f1, auc]\n",
    "parent_index = ['Acc', 'Prec', 'Recall', 'F1', 'AUC']\n",
    "child_index = ['Mean', 'Std']\n",
    "df = generate_multi_df(data, parent_index, child_index).T\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = ExperimentADeLEn(.1, .2, 2)\n",
    "result_dir = exp.config()['save_result_dir']\n",
    "df = pd.read_csv(os.path.join(result_dir, 'metrics.csv'), index_col=0)\n",
    "df['Accuracy'].mean(), df['Accuracy'].std(),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df['Accuracy'].mean(), df['Accuracy'].std()), (df['Precision'].mean(), df['Precision'].std()), (df['Recall'].mean(), df['Recall'].std()), (df['F1'].mean(), df['F1'].std()), (df['AUC'].mean(), df['AUC'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = ExperimentSupervised(.1, .2)\n",
    "result_dir = exp.config()['save_result_dir']\n",
    "df = pd.read_csv(os.path.join(result_dir, 'metrics.csv'), index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df['Accuracy'].mean(), df['Accuracy'].std()), (df['Precision'].mean(), df['Precision'].std()), (df['Recall'].mean(), df['Recall'].std()), (df['F1'].mean(), df['F1'].std()), (df['AUC'].mean(), df['AUC'].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_score_histogram(score_df:pd.DataFrame, normal_bins=10, anomaly_bins=10):\n",
    "    from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "    with plt.style.context('seaborn-colorblind'):\n",
    "        fig = plt.figure(figsize=(5, 4))\n",
    "        ax = plt.gca()\n",
    "        ax.hist(score_df['Normal'], bins=normal_bins, alpha=.7, label='Normal')\n",
    "        ax.hist(score_df['Anomaly'], bins=anomaly_bins, alpha=.7, label='Anomaly')\n",
    "        ax.legend(fontsize='x-large')\n",
    "        ax.xaxis.set_major_locator(MaxNLocator(nbins=5)) \n",
    "        ax.yaxis.set_major_locator(MaxNLocator(nbins=5))\n",
    "        ax.set_ylabel('Samples', fontsize='xx-large')\n",
    "        ax.set_xlabel('Score', fontsize='xx-large')\n",
    "        ax.tick_params(axis='both', which='major', labelsize='x-large')\n",
    "        ax.grid()\n",
    "        \n",
    "        return fig\n",
    "    \n",
    "def plot_reconstructed(model, r0=(-6, 6), r1=(-6, 6), n=12):\n",
    "    model.eval()\n",
    "    w = 28\n",
    "    img = np.zeros((n*w, n*w))\n",
    "\n",
    "    fig = plt.figure(figsize=(5, 5))\n",
    "\n",
    "    for i, y in enumerate(np.linspace(*r1, n)):\n",
    "        for j, x in enumerate(np.linspace(*r0, n)):\n",
    "            z = torch.Tensor([[x, y]])\n",
    "            x_hat = torch.tanh(model.decode_path(z)) # ADeLEn\n",
    "            x_hat = x_hat.reshape(w, w).to('cpu').detach().numpy()\n",
    "            img[(n-1-i)*w:(n-1-i+1)*w, j*w:(j+1)*w] = x_hat\n",
    "    \n",
    "    plt.xlabel('$\\mathcal{N}(0, \\sigma_1)$', fontsize='xx-large')\n",
    "    plt.ylabel('$\\mathcal{N}(0, \\sigma_2)$', fontsize='xx-large')\n",
    "    plt.tick_params(axis='both', which='major', labelsize='x-large')\n",
    "    plt.imshow(img, extent=[*r0, *r1], cmap='viridis')\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADeLEn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in [2, 5, 10]:\n",
    "    for pollution in pollution_exp:\n",
    "        exp = ExperimentADeLEn(.1, pollution, d)\n",
    "        result_dir = exp.config()['save_result_dir']\n",
    "        img_dir = exp.config()['save_imgs_dir']\n",
    "\n",
    "        df = pd.read_csv(os.path.join(result_dir, 'metrics.csv'), index_col=0)\n",
    "        idx = df['AUC'].apply(lambda x: abs(x - df['AUC'].mean())).idxmin()\n",
    "\n",
    "        score_df = pd.read_pickle(os.path.join(result_dir, 'sample_score.pkl'))[f'It {idx+1}']\n",
    "        fig = generate_score_histogram(score_df)\n",
    "        fig.savefig(os.path.join(img_dir, f'score_histogram_{pollution}.pdf'), bbox_inches='tight')\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pollution in pollution_exp:\n",
    "    exp = ExperimentADeLEn(.1, pollution, d=2)\n",
    "    img_dir = exp.config()['save_imgs_dir']\n",
    "    exp.run()\n",
    "\n",
    "    fig = plot_reconstructed(exp.model, r0=(-6, 6), r1=(-6, 6), n=10)\n",
    "    fig.savefig(os.path.join(img_dir, f'reconstruction_{pollution}.pdf'), bbox_inches='tight')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pollution in pollution_exp:\n",
    "    exp = ExperimentSupervised(.1, pollution)\n",
    "    result_dir = exp.config()['save_result_dir']\n",
    "    img_dir = exp.config()['save_imgs_dir']\n",
    "\n",
    "    df = pd.read_csv(os.path.join(result_dir, 'metrics.csv'), index_col=0)\n",
    "    idx = df['AUC'].apply(lambda x: abs(x - df['AUC'].mean())).idxmin()\n",
    "\n",
    "    score_df = pd.read_pickle(os.path.join(result_dir, 'sample_score.pkl'))[f'It {idx+1}']\n",
    "    score_df['Normal'] = score_df['Normal'].apply(lambda x: x.item() if x is not None else x)\n",
    "    score_df['Anomaly'] = score_df['Anomaly'].apply(lambda x: x.item() if x is not None else x)\n",
    "    \n",
    "    fig = generate_score_histogram(score_df, normal_bins=(1 if pollution==.2 else 10))\n",
    "    fig.savefig(os.path.join(img_dir, f'score_histogram_{pollution}.pdf'), bbox_inches='tight')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pollution in pollution_exp:\n",
    "    exp = ExperimentSVM(.1, pollution)\n",
    "    result_dir = exp.config()['save_result_dir']\n",
    "    img_dir = exp.config()['save_imgs_dir']\n",
    "\n",
    "    df = pd.read_csv(os.path.join(result_dir, 'metrics.csv'), index_col=0)\n",
    "    idx = df['AUC'].apply(lambda x: abs(x - df['AUC'].mean())).idxmin()\n",
    "\n",
    "    score_df = pd.read_pickle(os.path.join(result_dir, 'sample_score.pkl'))[f'It {idx+1}']\n",
    "    \n",
    "    fig = generate_score_histogram(score_df)\n",
    "    fig.savefig(os.path.join(img_dir, f'score_histogram_{pollution}.pdf'), bbox_inches='tight')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
